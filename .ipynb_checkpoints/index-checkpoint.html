<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>ECE 4160 - Fast Robots | Ivan Huang</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg bg-secondary text-uppercase fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand" href="#page-top">ECE 4160 - Fast Robots</a>
                <button class="navbar-toggler text-uppercase font-weight-bold bg-primary text-white rounded" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto">
                        <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded" href="#portfolio">Portfolio</a></li>
                        <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded" href="#about">About</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
        <header class="masthead bg-primary text-white text-center">
            <div class="container d-flex align-items-center flex-column">
                <!-- Masthead Avatar Image-->
                <img class="rounded-circle img-fluid w-25 img-centered" src="assets/img/portfolio/ivanhuang.png" alt="..." />
                <!-- Masthead Heading-->
                <br>
                <br>
                <h2 class="masthead-heading text-uppercase mb-0">ECE 4160 - Fast Robots</h2>
                <!-- Icon Divider-->
                <div class="divider-custom divider-light">
                    <div class="divider-custom-line"></div>
                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                    <div class="divider-custom-line"></div>
                </div>
                <!-- Masthead Subheading-->
                <p class="masthead-subheading font-weight-light mb-0">Author: Ivan Huang (NetID: qh229)
                </p>
            </div>
        </header>
        <!-- Portfolio Section-->
        <section class="page-section portfolio" id="portfolio">
            <div class="container">
                <!-- Portfolio Section Heading-->
                <h2 class="page-section-heading text-center text-uppercase text-secondary mb-0">Portfolio</h2>
                <!-- Icon Divider-->
                <div class="divider-custom">
                    <div class="divider-custom-line"></div>
                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                    <div class="divider-custom-line"></div>
                </div>
                <!-- Portfolio Grid Items-->
                <div class="row justify-content-center">
                    <!-- Portfolio Item 1-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal1"
                             style="background-color: #BA4C63; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid" src="assets/img/portfolio/Artemis.png" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 1</p>
                        </div>
                    </div>

                    <!-- Portfolio Item 2-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal2"
                             style="background-color: #1FA4B5; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid w-50" src="assets/img/portfolio/IMU.png" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 2</p>
                        </div>
                    </div>

                    <!-- Portfolio Item 3-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal3"
                             style="background-color: #130F0F; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid w-75" src="assets/img/portfolio/TF.png" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 3</p>
                        </div>
                    </div>

                    <!-- Portfolio Item 4-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal4"
                             style="background-color: #1D83C1; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid w-75" src="assets/img/portfolio/Motor.png" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 4</p>
                        </div>
                    </div>

                    <!-- Portfolio Item 5-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal5"
                             style="background-color: #1037B5; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid w-75" src="assets/img/portfolio/pid.png" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 5</p>
                        </div>
                    </div>

                    <!-- Portfolio Item 6-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal6"
                             style="background-color: #CCB2A3; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid w-75" src="assets/img/portfolio/orien_pic.webp" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 6</p>
                        </div>
                    </div>

                    <!-- Portfolio Item 7-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal7"
                             style="background-color: #BA4C63; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid" src="assets/img/portfolio/kf.png" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 7</p>
                        </div>
                    </div>

                    <!-- Portfolio Item 8-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal8"
                             style="background-color: #1FA4B5; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid w-75" src="assets/img/portfolio/stunt.png" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 8</p>
                        </div>
                    </div>

                    <!-- Portfolio Item 9-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal9"
                             style="background-color: #CCB2A3; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid w-75" src="assets/img/portfolio/map_global_wall.png" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 9</p>
                        </div>
                    </div>

                    <!-- Portfolio Item 10-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal10"
                             style="background-color: #1D83C1; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid" src="assets/img/portfolio/local_traj3.png" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 10</p>
                        </div>
                    </div>

                    <!-- Portfolio Item 11-->
                    <div class="col-md-6 col-lg-4 mb-5">
                        <div class="portfolio-item mx-auto d-flex flex-column justify-content-center align-items-center p-3" 
                             data-bs-toggle="modal" data-bs-target="#portfolioModal11"
                             style="background-color: #1037B5; border-radius: 10px; padding: 20px; text-align: center; height: 300px; width: 100%;">
                            
                            <div class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                                <div class="portfolio-item-caption-content text-center text-white">
                                    <i class="fas fa-plus fa-3x"></i>
                                </div>
                            </div>
                            
                            <!-- Centered Image -->
                            <img class="img-fluid w-75" src="assets/img/portfolio/local_bel_00.png" 
                                 alt="..." style="max-width: 80%; max-height: 70%; object-fit: contain; display: block; margin: 0 auto;">
                            
                            <!-- Bold, Centered Text Below Image -->
                            <p class="fw-bold mt-4 mb-2 text-center" style="color: white; font-size: 32px;">Lab 11</p>
                        </div>
                    </div>

                </div>
            </div>
        </section>
        <!-- About Section-->
        <section class="page-section bg-primary text-white mb-0" id="about">
            <div class="container">
                <!-- About Section Heading-->
                <h2 class="page-section-heading text-center text-uppercase text-white">About</h2>
                <!-- Icon Divider-->
                <div class="divider-custom divider-light">
                    <div class="divider-custom-line"></div>
                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                    <div class="divider-custom-line"></div>
                </div>
                <!-- About Section Content-->
                <div class="row justify-content-center">
                    <div class="col-lg-6 ">
                        <p class="lead">I am a current Undergraduate of Engineering student at Cornell University majoring in Electrical 
                            and Computer Engineeering. My area of interests include robotics, automotives, embedded systems, 
                            microcontrollers, hardware, and power systems. I can be reached at qh229@cornell.edu.
        </p></div>       
        </section>

        <!-- Copyright Section-->
        <div class="copyright py-4 text-center text-white">
            <div class="container"><small>Copyright &copy; Ivan Website 2025</small></div>
        </div>

        <!-- Portfolio Modals-->
        <!-- Portfolio Modal 1-->
        <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" aria-labelledby="portfolioModal1" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 1<h3 class="text-warning">The Artemis board and Bluetooth</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5" src="assets/img/portfolio/Artemis.png" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Introduction</h5>
                                        <p>
                                            During this lab, I explored two major tasks:
                                        </p>
                                        <ol>
                                            <li><strong>Lab 1A:</strong> Familiarized with the SparkFun RedBoard Artemis Nano using the Arduino IDE.</li>
                                            <li><strong>Lab 1B:</strong> Established Bluetooth communication between Artemis board and a Python
                                                 environment running in a Jupyter notebook.</li>
                                        </ol>
                                        <p>
                                            By the end of these two lab sections, I set up my board, gathered sensor data, and used wireless 
                                            connections to transmit information back and forth.
                                        </p>
                                        <br>

                                        <h5>Parts Required</h5>
                                        <ul>
                                            <li>1 × SparkFun RedBoard Artemis Nano</li>
                                            <li>1 × USB Cable</li>
                                        </ul>
                                        <br>

                                        <h5>Lab 1A: Arduino IDE and Artemis Board</h5>
                                        <p>
                                            <strong>Objective:</strong><br/>
                                            1. Install the <a href="https://www.arduino.cc/en/software">Arduino IDE</a> and 
                                            <a href="https://learn.sparkfun.com/tutorials/artemis-development-with-the-arduino-ide/setting-up-the-arduino-ide">Sparkfun Appollo3 boards</a>
                                             manager to program the Artemis board.<br/>
                                            2. Upload example sketches to test the board's built-in LED, serial communication, temperature 
                                            sensor, and pulse-density microphone.
                                        </p>

                                        
                                        <p>
                                            <strong>Prelab:</strong><br/>
                                            In preparation, I installed the Arduino IDE (latest version), added the SparkFun Apollo3 boards 
                                            manager via the SparkFun JSON link, and checked that my board manager was up to date.
                                        </p>

                                        
                                        <p>
                                            <strong>Tasks:</strong><br/>
                                        </p>

                                        <ol>
                                            <li>Selected the correct board and port in the Arduino IDE and connect to Artemis board.</li>
                                            <li>
                                                Uploaded the following example sketches from <code>File -&gt; Examples</code>:
                                                <ul>
                                                    <li>
                                                        <strong>Blink</strong> - Confirmed the onboard LED was blinking.
                                                        <video width="400" controls class="d-block mx-auto mt-2">
                                                            <source src="assets/img/portfolio/blink.mp4" type="video/mp4">
                                                        </video>
                                                    </li>
                                                    <br>
                                                    <li>
                                                        <strong>Example4_Serial</strong> - Printed messages to the Serial Monitor and tested input/output.
                                                    </li>
                                                    <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/serial.png" alt="Serial Monitor Example">
                                                    <br>
                                                    <li>
                                                        <strong>Example2_analogRead</strong> - Monitored the onboard temperature sensor.
                                                    </li>
                                                    <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/temperature.png" alt="Analog Read Example">
                                                    <br>
                                                    <li>
                                                        <strong>Example1_MicrophoneOutput</strong> - Tested the pulse density microphone.
                                                        <video width="650" controls class="d-block mx-auto mt-2">
                                                            <source src="assets/img/portfolio/microphone.mp4" type="video/mp4">
                                                        </video>                                                    
                                                    </li>
                                                </ul>
                                            </li>
                                        </ol>

                                        
                                        <br>
                                        <br>
                                        
                                        <h5>Lab 1B: Bluetooth Connectivity</h5>
                                        <p>
                                            <strong>Objective:</strong><br/>
                                            1. Establish BLE (Bluetooth Low Energy) communication between Artemis board and a Python environment in Jupyter Lab.<br/>
                                            2. Send commands from computer to the Artemis and receive data back from the board.
                                        </p>

                                        <p>
                                            <strong>Prelab:</strong><br/>
                                            Before starting, I read through and understand the provided Python code (in <code>ble_python</code>), 
                                            which uses <strong>ArduinoBLE</strong> on the Artemis side. I noted that message sizes are limited 
                                            to <strong>150 bytes</strong>.
                                        </p>

                                        
                                        <p>
                                            <strong>Setup and Configuration:</strong>
                                            <p>
                                            After setting up the virtual environment in my file directory and install jupyter lab through pip, 
                                            I was able to open the given <code>ble_robot_1.2</code> successfully. 
                                            </p>
                                        </p>

                                        <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/Jupyter1.png" alt="...">
                                        <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/Jupyter2.png" alt="...">
                                        <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/Jupyter3.png" alt="...">
                                        <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/Jupyter4.png" alt="...">
                                        <br>

                                        <p>
                                            To set up BLE connection with the Artemis, I retrieved my Artemis's MAC Address from the serial 
                                            output, updated <code>MAC</code> address in <code>connections.yaml</code>, and generated a new 
                                            <code>UUID</code> to prevent conflicts. 
                                        </p>
                                        <img class="img-fluid w-50 d-block mx-auto mt-2" src="assets/img/portfolio/MAC.png" alt="...">
                                        <br>
                                        <img class="img-fluid w-50 d-block mx-auto mt-2" src="assets/img/portfolio/uuid.png" alt="...">
                                        <br>
                                        <p>
                                            After updating <code>UUIDs</code> in <code>ble_arduino.ino</code> and <code>ble_serice</code> ID in 
                                            <code>connections.yaml</code>, changing the <code>elif</code> condition in <code>base_ble.py</code> 
                                            line 69, I successfully connected to the Artemis board through bluetooth.
                                        </p>
                                        <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/BLEconnect.png" alt="...">
                                        <br>
                                        <br>
                                        
                                        <p>
                                            <strong>Tasks:</strong>
                                        </p>

                                        <ol>
                                            <li>
                                                <strong>ECHO Command:</strong> 
                                                <br>
                                                <p>
                                                    This task involves sending an <strong>ECHO</strong> command with a string value from the computer to the Artemis. 
                                                    The Artemis receives the command and returns an augmented string back to the computer.
                                                    As shown in the images below, the Python code uses <code>ble.send_command(CMD.ECHO, "HiHello")</code> to send the string "HiHello" to the board. 
                                                    On the Artemis side, the C++ firmware reads this string with <code>robot_cmd.get_next_value()</code>, prepends "Robot says -&gt; ", 
                                                    appends a smiley, and writes it to the BLE characteristic. Finally, the Python script calls <code>ble.receive_string(...)</code> 
                                                    to capture the modified string, printing the returned message.
                                                </p>
                                                <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/Echo.png" alt="...">
                                                <br>
                                                <img class="img-fluid w-50 d-block mx-auto mt-2" src="assets/img/portfolio/Echo_test.png" alt="...">
                                                <br>
                                            </li>


                                            <li>
                                                <strong>SEND_THREE_FLOATS:</strong>
                                                <br>
                                                <p>
                                                    The <strong>SEND_THREE_FLOATS</strong> command allows the computer to transmit three 
                                                    floating‐point values to the Artemis board, where they are then parsed and displayed.
                                                    In the C++ firmware, each float is retrieved one at a time via 
                                                    <code>robot_cmd.get_next_value()</code> (e.g. <code>float_a</code>, 
                                                    <code>float_b</code>, <code>float_c</code>). If any retrieval fails, the function 
                                                    returns early, ensuring valid data is received. Once all three floats are successfully 
                                                    parsed, a confirmation message is printed to the Serial Monitor with the values.
                                                </p>
                                                <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/sendThreeFloat.png" alt="...">
                                                <br>
                                                <img class="img-fluid w-50 d-block mx-auto mt-2" src="assets/img/portfolio/sendThreeFloat_test.png" alt="...">
                                                <br>
                                            </li>
                                            <li>
                                                <strong>GET_TIME_MILLIS:</strong>
                                                <br>
                                                <p>
                                                    The <strong>GET_TIME_MILLIS</strong> command prompts the Artemis board to reply 
                                                    with the current milliseconds count since startup. In the C++ firmware, this is 
                                                    done by calling the <code>millis()</code> function, converting its return value 
                                                    to an integer, and appending it to a string prefixed with 
                                                    <code>"T:"</code>. 
                                                </p>
                                                <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/getTimeMillis.png" alt="...">
                                                <br>
                                                <img class="img-fluid w-50 d-block mx-auto mt-2" src="assets/img/portfolio/getTimeMillis_test.png" alt="...">
                                                <br>
                                            </li>
                                            <li>
                                                <strong>NOTIF_HANDLER:</strong>
                                                <br>
                                                <p>
                                                    To receive the string value from the Artemis board via the
                                                    <code>BLEStringCharacteristic</code>, a notification handler must be registered in
                                                    Python. This is typically done by calling
                                                    <code>ble.start_notify(ble.uuid['RX_STRING'], notif_handler)</code>, where
                                                    <em>notif_handler</em> is a callback function. Whenever the Artemis board writes
                                                    a new value to the RX_STRING characteristic, the BLE stack automatically invokes
                                                    <em>notif_handler</em>, passing in the raw bytes. 
                                                </p>
                                                <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/notifHandler.png" alt="...">
                                                <br>
                                            </li>
                                            <li>
                                                <strong>NOTIF_HANDLER:</strong>
                                                <br>
                                                <p>
                                                    The board will repeatedly retrieves the current time in milliseconds
                                                    using <code>millis()</code> inside a <em>for</em> loop, then appends that value to
                                                    a string (e.g., <code>"T:2839.000"</code>) and sends it to the laptop using the BLE
                                                    characteristic. The Python side has a notification handler listening for updates on
                                                    <code>RX_STRING</code>, which logs each new message arrival. By collecting these
                                                    values for several seconds and noting their arrival timestamps in Python, I
                                                    calculated the frequently messages is <strong>73 message per second</strong>. 
                                                </p>
                                                <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/getTimeMillisLoop.png" alt="...">
                                                <br>
                                                <img class="img-fluid w-50 d-block mx-auto mt-2" src="assets/img/portfolio/getTimeMillisLoop_test.png" alt="...">
                                                <br>
                                            </li>
                                            <li>
                                                <strong>Store Timestamps in an Array:</strong>
                                                <br>
                                                <p>
                                                    To collect multiple timestamps on the Artemis
                                                    board and store them for later transmission, first, a global array (e.g. 
                                                    <code>timestamps[SIZE]</code>) is defined so it can be accessed by different commands.
                                                    The helper function <strong>TIME_DATA_ARRAY</strong> reads two parameters—an index 
                                                    (how many timestamps to store) and an optional delay between reads. In a loop, 
                                                    <code>millis()</code> is called to get the current time in milliseconds, and that 
                                                    timestamp is placed into the array. A brief delay (<code>delay(timestamp_delay)</code>) 
                                                    is used to select the data collection gap to widen the data collection time range.
                                                  </p>
                                                  <p>
                                                    Next, the <strong>SEND_TIME_DATA</strong> function reads in how many timestamps need 
                                                    to be transmitted, then loops through the array and sends each entry as a string 
                                                    (e.g., <code>"T:6715"</code>) using the BLE characteristic. 
                                                  </p>
                                                <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/sendTimeData.png" alt="...">
                                                <br>
                                                <img class="img-fluid w-50 d-block mx-auto mt-2" src="assets/img/portfolio/sendTimeData_test.png" alt="...">
                                                <br>
                                            </li>
                                            <li>
                                                <strong>Store Timestamps and Temperature:</strong>
                                                <br>
                                                <p>
                                                    In this extension of the previous data collection task, a second global array is 
                                                    created to store temperature readings in parallel with the existing timestamp array. 
                                                    Each element in the temperature array corresponds to the same index in the timestamp 
                                                    array—so for index <em>i</em>, the recorded time and temperature values were 
                                                    captured simultaneously.
                                                </p>
                                                <p>
                                                    A new command (<strong>TIME_TEMP_DATA_ARRAY</strong>) collects a specified number of 
                                                    measurements, storing the current millisecond count (using <code>millis()</code>) in 
                                                    one array and the temperature reading (for instance, from 
                                                    <code>getTempDegF()</code>) in the other. A second command 
                                                    (<strong>GET_TEMP_READINGS</strong>) then loops through both arrays concurrently, 
                                                    generating a string (for example, <code>"Time:93512  Temp:77"</code>) for each entry 
                                                    and sending it over BLE to the host computer.
                                                </p>
                                                <img class="img-fluid w-75 d-block mx-auto mt-2" src="assets/img/portfolio/timeTempData.png" alt="...">
                                                <br>
                                                <img class="img-fluid w-50 d-block mx-auto mt-2" src="assets/img/portfolio/timeTempData_test.png" alt="...">
                                                <br>
                                            </li>
                                            <li>
                                                <strong>Discussion of Two Methods:</strong> 
                                                <p>
                                                    Storing data in arrays before transmission (Method B) allows for higher sampling
                                                    rates and more efficient BLE throughput compared to sending each sample immediately
                                                    (Method A). Method A is simpler and provides near-real-time feedback but can overwhelm
                                                    the BLE connection if samples are frequent, while Method B can accumulate data and
                                                    then send it in bursts, albeit requiring more memory and delaying data visibility.
                                                    For example, the Artemis board's 384 kB of RAM can store tens of thousands of pairs
                                                    of 32-bit time stamps and temperature readings (roughly 8 bytes per sample), making
                                                    this approach ideal for high-speed data logging followed by bulk data transfer when
                                                    needed.
                                                </p>
                                            </li>
                                        </ol>

                                        <br>

                                        <h5>Conclusion</h5>
                                        <p>
                                            In Lab 1A, I installed and configured the Arduino IDE (v2.0) along with the SparkFun Apollo3
                                            boards package to program my SparkFun Artemis board. This allowed me to verify fundamental
                                            functionalities—such as blinking the built-in LED and reading basic sensor values. In Lab 1B,
                                            I set up a BLE link using the SparkFun Artemis BLE Command library on the board side and a
                                            custom Python script running in Jupyter Lab on the computer side, with a BLE helper file
                                            (<code>ble.py</code>) managing the connection. With this setup, I successfully transmitted
                                            commands (like <code>ECHO</code> and <code>GET_TIME_MILLIS</code>) and received data
                                            wirelessly, creating a strong foundation for future labs where I plan to collect sensor
                                            readings, control motors, and debug without a wired connection.
                                        </p>
                                        <p>
                                            I faced several challenges such as a library path mismatch in the Arduino IDE, an outdated
                                            pip version that broke Jupyter Lab, and the steep learning curve of creating a portfolio
                                            website. Resolving these issues was time-consuming but ultimately improved my troubleshooting
                                            skills for embedded toolchains, Python environments, and basic web development. These labs
                                            solidified my understanding of BLE communication on the Artemis platform and underscored the
                                            importance of a well-configured development environment.
                                        </p>

                                        <br>

                                        <h5>Appendices</h5>
                                        <p>
                                            <strong>Appendix: References</strong><br/>
                                            - SparkFun RedBoard Artemis Nano<br/>
                                            - SparkFun Forums<br/>
                                            - ArduinoBLE Library
                                        </p>
                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio Modal 2-->
        <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" aria-labelledby="portfolioModal2" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 2<h3 class="text-warning">IMU Data Collection and Communication</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5  w-25" src="assets/img/portfolio/IMU.png" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Objective</h5>
                                        <p>The objective of this lab is to integrate the IMU sensor with the Artemis board, establish Bluetooth communication, collect and process IMU data (accelerometer and gyroscope), and transmit the recorded data wirelessly. Filtering techniques were applied to enhance data quality.</p>

                                        <h5>Materials/Software</h5>
                                        <ul>
                                            <li>1x SparkFun RedBoard Artemis Nano</li>
                                            <li>1x USB A to C Cable</li>
                                            <li>1x IMU Sensor (ICM-20948)</li>
                                            <li>1x Quik Connector</li>
                                            <li>Arduino IDE, JupyterLab, Bluetooth-enabled Device</li>
                                        </ul>

                                        <h5>System Setup</h5>
                                        <p>The IMU sensor was connected to the Artemis board using Quik connectors. The Arduino IDE facilitated programming, while JupyterLab handled data visualization.</p>
                                        <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/IMU_setup.jpg" alt="...">
                                        

                                        <br>

                                        <p>
                                            <strong>Accelerometer:</strong><br/>
                                            By applying the <code>atan2</code> function along with the <code>M_PI</code> constant from the <code>math.h</code> library, we calculated pitch and roll using the 
                                            formulas: <strong>Pitch (θ) = atan2(accY, accZ) * (180 / M_PI)</strong> and <strong>Roll (φ) = atan2(accX, accZ) * (180 / M_PI)</strong>. These calculations were 
                                            validated through controlled positioning at -90°, 0°, and 90° using table surfaces as reference points. 
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/acc_code_raw.png" alt="...">
                                        <video width="400" controls class="d-block mx-auto mt-2">
                                            <source src="assets/img/portfolio/acc_pitch_roll.mp4" type="video/mp4">
                                        </video>
                                        <br>

                                        <p>
                                            To filter out noise, we analyzed the frequency spectrum of the data using the Fast Fourier Transform (FFT). I stored the accelerometer data into array and outputed the raw 
                                            data over through BLE, then collect them into array on my laptop to be analysis in Jupyter Lab. This analysis helped identify the dominant frequencies, allowing us to 
                                            determine an appropriate cutoff frequency for a low-pass filter.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/accel_data_collect_code.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/accel_fft.png" alt="...">
                                        <p>
                                            we observed that the accelerometer readings were close to zero, as expected, but accompanied by some noise. We selected a cutoff frequency of <strong>10Hz</strong>, 
                                            and implemented a low-pass filter to suppress high-frequency noise, with the alpha constant calculated using the formula <strong>alpha = dt / (RC + dt)</strong>, 
                                            where <strong>RC = 1 / (2 * π * cutoff frequency)</strong>. The final <strong>alpha = 0.213</strong>. We plotted both the original and filtered accelerometer data 
                                            to compare their performance. The filtered signal showed reduced noise and more stable readings, especially under vibrational disturbances. 
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/accel_fft_lpf.png" alt="...">
                                        <br>

                                        
                                        <p>
                                            <strong>Gyroscope:</strong><br/>
                                            The <code>pitch</code>, <code>roll</code>, and <code>yaw</code> of the gyroscope were determined using equations covered in the lectures. Since the gyroscope measures angular velocities, integrating these 
                                            values yields the corresponding tilt angles. Compared to the accelerometer, the gyroscope's output exhibited significantly <strong>less noise</strong>. However, the gyroscope readings 
                                            tended to <strong>drift over time</strong> instead of stabilizing at a fixed point, likely due to the accumulation of noise. Additionally, the gyroscope requires an initial reference 
                                            value to generate accurate sensor readings.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/gyro_code.png" alt="...">
                                        <img class="img-fluid rounded mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/gyro_data_plot.png" alt="...">
                                        <br>

                                        <p>
                                            <strong>Complementary Filter:</strong><br/>
                                            By applying a complementary filter with a <strong>coefficient of 0.35</strong> and combining the accelerometer and gyroscope readings, accurate <code>roll</code> and <code>pitch</code> angles were calculated, 
                                            as shown below. The integration of sensor fusion with the filter significantly enhances the <strong>stability</strong> of the sensor. The results and practical application of this fusion are presented as follows:
                                        </p>
                                        <img class="img-fluid rounded w-75mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/Compl_pitch_roll.png" alt="...">
                                        <br>

                                        
                                        
                                        <h5>Sampleing Data</h5>
                                        <p>
                                            This lab focused on optimizing IMU data collection, processing, and wireless transmission using the Artemis board. To improve main loop efficiency, IMU data conversion and storage were modularized into 
                                            the <code>storeData</code> function, with an <code>IMUDataReady</code> signal tracking processing status. Data is collected via <code>myICM.AGMT()</code> when the IMU is ready (<code>myICM.dataReady</code>), 
                                            recording is active (<code>recording</code>), and prior data is stored (<code>IMUDataReady</code>). After new data is collected, <code>IMUDataReady</code> is set to <code>false</code>, triggering 
                                            <code>storeData</code> to process and store the data, then reset the signal.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/BLE_data_loop_code.png" alt="...">
                                        <p>
                                            This <strong>non-blocking</strong> design eliminates explicit data wait times, allowing the loop to continuously check <code>myICM.dataReady</code> and <code>IMUDataReady</code>. This approach <strong>doubles 
                                            the loop's speed</strong> compared to the IMU's processing rate.
                                        </p>
                                        <img class="img-fluid rounded mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/BLE_data_loop_rate.png" alt="...">
                                        <p>
                                            Data is stored in a <strong>2500 x 10 2D array</strong> (<code>imu_data1</code>), consolidating timestamps, accelerometer, gyroscope, and complementary <code>Pitch</code>/<code>Roll</code> data. If data 
                                            collection ends before the array is full, a <code>for</code> loop transmits all entries via BLE. When data exceeds the array's capacity, a circular buffer overwrites the oldest data after transmission, 
                                            slightly slowing storage but maintaining continuous collection without extra arrays.
                                        </p>
                                        <p>
                                            After connected the <code>Artemis board</code> and <code>IMU sensor</code> via <code></code>BLE, we successfully requested <strong>10s</strong> (within <code>imu_data1</code> data size) and <strong>20s</strong> 
                                            (within <code>imu_data1</code> data size) of IMU data and printed them on Jupyter Lab using the <code>notif_handler</code> implemented in Lab1:   
                                        </p>
                                        <img class="img-fluid rounded mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/BLE_data_10s_20s.png" alt="...">
                                        <img class="img-fluid rounded mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/BLE_data_10s_20s_print.png" alt="...">
                                        <img class="img-fluid rounded mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/BLE_data_10s_20s_print1.png" alt="...">
                                        <p>
                                            We using double-precision format ensures high accuracy, which is critical for motion tracking and filter calculations. After testing the sampling function, we conclude that the Artemis board's 384 KB SRAM supports 
                                            around 20 seconds of high-rate IMU data (~2600 samples) before the <code>IMU_data1</code> array full up, then continue recording using array element recycling unil desired time ended. Within the <strong>20 seconds</strong> interval, 
                                            over <strong>2,600 data points</strong> were captured and successfully <strong>transmitted</strong> with additional <strong>2 seconds</strong> via Bluetooth, with verified timestamp accuracy and stable communication throughout.
                                        </p>

                                        <br>
                                        <br>

                                        <h5>Record a stunt</h5>
                                        <p>
                                            In the final part of the lab, we mounted the battery onto the RC car, spent about five minutes manually controlling the car in an open hallway to observe its performance. The car exhibited strong acceleration with a 
                                            slight delay in braking response. It handled turns responsively, although sharp turns or braking at high speeds often led to oversteering. 
                                        </p>
                                        <video width="400" controls class="d-block mx-auto mt-2">
                                            <source src="assets/img/portfolio/RCstunt.mp4" type="video/mp4">
                                        </video>
                                        <br>
                                        <br>

                                        <h5>Conclusion</h5>
                                        <p>
                                            Overall, the lab objectives were successfully met. The optimized main loop allowed for efficient <strong>data collection</strong>, while the <strong>single-array</strong> data structure ensured synchronized storage and easy transmission over 
                                            Bluetooth. The system demonstrated its ability to capture and send over <strong>5</strong> and even <strong>20</strong> seconds of IMU data reliably. Additionally, the RC car's performance tests provided essential baseline 
                                            data, laying the groundwork for future autonomous driving experiments.
                                        </p>
                                        <br>
                                        
                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio Modal 3-->
        <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" aria-labelledby="portfolioModal3" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 3<h3 class="text-warning">Time of Flight Sensors</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5  w-25" src="assets/img/portfolio/TF.png" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Objective</h5>
                                        <p>
                                            The purpose of this lab is to equip the robot with distance sensors, as faster sampling and reliable sensor readings enhance the robot's ability to navigate effectively.
                                        </p>
                                        <br>

                                        <h5>Materials</h5>
                                        <ul>
                                            <li>1 x SparkFun RedBoard Artemis Nano</li>
                                            <li>1 x USB cable</li>
                                            <li>2 x 4m ToF sensors</li>
                                            <li>1 x 9DOF IMU sensor</li>
                                            <li>1 x QWIIC Breakout board</li>
                                            <li>3 x Qwiic connectors</li>
                                            <li>1 x 650mAh battery from the RC car</li>
                                            <li>1 x Ruler or graph paper</li>
                                            <li>1 x Wire strippers/ Mini cutters</li>
                                            <li>1 x Soldering iron</li>
                                        </ul>
                                        <br>

                                        <h5>Battery and ToF Sensor Setup</h5>
                                        <p>
                                            Following the instructions, I successfully powered the Artemis board using a 650mAh battery from an RC car. First, I prepared a JST connector and carefully cut 
                                            the battery wires one at a time to prevent short circuits. After soldering the battery wires to the JST jumper wires, I applied heat shrink tubing for insulation. 
                                            Before connecting the battery to the Artemis, I verified the polarity to ensure a safe connection. Once everything was secured, I powered up the Artemis using only 
                                            the battery and confirmed that the Artemis was powered and working properly with its blue indicating LED. 
                                        </p>
                                        <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/battery.jpg" alt="...">
                                        <video width="400" controls class="d-block mx-auto mt-2">
                                            <source src="assets/img/portfolio/battery_artemis.mp4" type="video/mp4">
                                        </video>
                                        <br>
                                        <p>
                                            Next, I set up the ToF sensors by installing the <strong>SparkFun VL53L1X 4m laser distance sensor library</strong> via the Arduino Library Manager. Using the ToF 
                                            sensor connection schematic on Pololu, I connected the <strong>QWIIC breakout board</strong> to the Artemis and wired the first ToF sensor by cutting a QWIIC cable 
                                            and soldering it to the sensor, ensuring the correct <strong>SDA/SCL</strong> connections. 
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/RC_tof.jpg" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/TF_artemis.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_imu_setup.jpg" alt="...">
                                        <br>
                                        <br>

                                        <h5>Instruction</h5>
                                        <p>
                                            <strong>Setting Up the ToF Sensors</strong>
                                            <p>
                                                According to the datasheet, the default address of the ToF sensor is listed as <strong>0x56</strong>, but when scanned, the detected address is <strong>0x29</strong>. 
                                                This difference occurs because I2C uses a <strong>7-bit addressing scheme</strong>, whereas some datasheets provide an <strong>8-bit address</strong>. The default 
                                                <strong>0x56</strong> represents the <strong>8-bit write address</strong>, which corresponds to <strong>0x2B</strong> in <strong>7-bit format</strong>. When scanned, 
                                                the <strong>0x29</strong> address is the actual <strong>7-bit device address</strong>. Since I2C automatically shifts this address left by one bit for read and write 
                                                operations, the complete <strong>8-bit address range</strong> becomes <strong>0x52</strong> for write operations and <strong>0x53</strong> for read operations.
                                            </p>
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/ToF_i2c.png" alt="...">
                                        <br>

                                        <p>
                                            <strong>Testing ToF Sensor Performance</strong>
                                            <p>        
                                                The ToF sensor offers three distance modes, each tailored for different sensing ranges and accuracy levels. Since I will operate the robot at a relatively <strong>slow speed</strong>,
                                                to detect obstacles and adjust its movement accordingly only requires 1.3 meter sensing range for the ToF sensor in the front of the robot and 0.5 meter sensing range for 
                                                the ToF at the side, the <strong>Short Mode (1.3m)</strong> is the most suitable choice. The two ToF sensors is set to different chosen mode by applying 
                                                <code>.setDistanceModeShort()</code>.
                                            </p>
                                        </p>
                                        <p>
                                            To test the ToF sensor <strong>Short Mode</strong>, I uploaded the <code>Example1_ReadDistance</code> from library to measure sensor data and compare with the actual 
                                            distance. Set up and the result plot are shown below:
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_measure_setup.png" alt="...">
                                        <img class="img-fluid rounded w-100 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_actual_measure.png" alt="...">
                                        <p>
                                            The plot shows a generally linear relationship between the actual and measured distances of the ToF sensor, but slight deviations and noise are present.  At shorter distances, the measurements are more precise, but as the range increases, small inconsistencies emerge due to signal attenuation, interference, and potential non-linearity in the sensor’s response. Additionally, external noise from ambient light and temperature fluctuations can affect readings, causing minor variations.
                                        </p>
                                        <br>

                                        <p>
                                            <strong>Using Two ToF Sensors Simultaneously</strong>
                                            <p>
                                                To run the two ToF sensors in parallel, it is necessary to configure each sensor with a unique I2C address. This requires connecting a digital wire from the Artemis to one of the ToF sensors to control its shutdown via the <strong>XSHUT</strong> pin while reassigning the I2C address of the other sensor. I used <strong>pin 8</strong> on the Artemis for this purpose.
                                            </p> 
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof2_setup_code.png" alt="...">
                                        <p>
                                            I also wrote a <code>tof1_tof2_data_serial</code> function to verify the 2 sensors parallel read by outputing the both sensor result data to the serial monitor.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof1_tof2_code.png" alt="...">
                                        <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof1_tof2_data.png" alt="...">
                                        <br>

                                        <p>
                                            To speed up the ToF sensor loop execution, I used <code>tof1.checkForDataReady()</code> routine to check for ToF sensor data in a if statment, which calls a <code>tof_data_store</code> function to store the value in an data array.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_data_fast.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_data_freq.png" alt="...">
                                        <p>
                                            The output in the serial monitor after collecting 2 seconds of ToF sensor data indicates that the main loop is <strong>10 times faster</strong> than the ToF sensor datarate.
                                        </p>
                                        <br>

                                        <p>
                                            <strong>Recording and Transmitting ToF & IMU Data</strong>
                                            <p>
                                                I integrated the Bluetooth sensor data transmission function into the ToF data collection logic to streamline future robot design implementation. Using the same approach as IMU data transmission, I first created an array to store ToF sensor readings and then sent the data as an array either when execution completed or when the Artemis data array reached capacity while reading new values.
                                            </p>
                                        </p>
                                        <p>
                                            Since the IMU extracts data much faster than the ToF sensor, ToF data transmission was frequently interrupted by IMU data transmission. To address this, I combined both IMU and ToF sensor data storage and transmission into a single function, using the IMU data rate to regulate data collection by checking <code>myICM.dataReady()</code>. This solution effectively mitigated the slow ToF data rate issue by transmitting previous ToF sensor readings when new ToF data was unavailable but new IMU data was detected.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/BLE_tof_code.png" alt="...">
                                        <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/BLE_tof_data.png" alt="...">
                                        <br>

                                        <p>
                                            Using the python <code>data_collect()</code> function implemented in jupyter lab, I connected the Artemis with IMU and 2 ToF sensors and collected their data through <strong>BLE</strong>, and
                                            generate plots for the IMU and ToF sensor data for 2 seconds. 
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/data_collect_jupyter.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/sensor_data_raw.png" alt="...">
                                        <img class="img-fluid rounded w-100 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/sensor_data_imu.png" alt="...">
                                        <img class="img-fluid rounded w-100 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/sensor_data_tof.png" alt="...">
                                        <p>
                                            I faced the 2 ToF sensors in different directions with different closest obstacle distances, which received 2 different means.
                                        </p>
                                        <br>


                                        <h5>Conclusion</h5>
                                        <p>
                                            In conclusion, I successfully integrated <strong>ToF sensors</strong> into the robot for obstacle detection, emphasizing the importance of sensor placement, efficient addressing methods, and optimized code execution. Using shutdown pins enabled effective dual sensor management, while selecting the <strong>Short mode (1.3m)</strong> provided a balanced tradeoff between range and accuracy. The implementation of asynchronous data retrieval allowed for efficient processing without blocking execution, ensuring <strong>real-time sensor ToF and IMU</strong> data updates over BLE. Future improvements could focus on refining sensor positioning to minimize blind spots and enhancing data processing techniques for even faster navigation response times.
                                        </p>
                                        <br>
                                        
                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio Modal 4-->
        <div class="portfolio-modal modal fade" id="portfolioModal4" tabindex="-1" aria-labelledby="portfolioModal4" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 4<h3 class="text-warning">Motor Driver and Open Loop Control</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5 w-25" src="assets/img/portfolio/Motor.png" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Objective</h5>
                                        <p>
                                            This lab focuses on transitioning from manual to open-loop control of a car using the SparkFun RedBoard Artemis Nano and two dual motor drivers. The objective is to program the car 
                                            to execute a predefined set of movements, demonstrating an understanding of PWM signal generation, motor driver control, and power management.
                                        </p>
                                        <br>

                                        <h5>Materials</h5>
                                        <ul>
                                            <li>1 x SparkFun RedBoard Artemis Nano</li>
                                            <li>1 x USB cable</li>
                                            <li>2 x 4m ToF sensors</li>
                                            <li>1 x 9DOF IMU sensor</li>
                                            <li>1 x QWIIC Breakout board</li>
                                            <li>3 x Qwiic connectors</li>
                                            <li>1 x JST2 connector+cable</li>
                                            <li>1 x Force1 RC car</li>
                                            <li>1 x Li-Ion 3.7V 850mAh battery</li>
                                            <li>2 x Dual motor drivers</li>
                                            <li>1 x Wire strippers/ Mini cutters</li>
                                            <li>1 x Soldering iron</li>
                                            <li>1 x External power supply</li>
                                            <li>1 x Oscilloscope</li>
                                        </ul>
                                        <br>

                                        <h5>Motor Driver and Battery Setup</h5>
                                        <p>
                                            To ensure operational stability, the Artemis board and the motor drivers are powered by separate batteries. This separation is crucial to:
                                        </p>
                                        <ul>
                                            <li>Protect the microcontroller from electrical noise generated by the motors.</li>
                                            <li>Maintain stable power delivery to both components.</li>
                                            <li>Reduce interference that can affect signal integrity and motor performance.</li>
                                        </ul>
                                        <p>
                                            From the Artemis pinout diagram, PWM-enabled pins (denoted by ‘~’) were selected for motor control:
                                        </p>
                                        <ul>
                                            <li>Motor Driver Left: Pin 3 (AB1IN_LEFT), Pin 14 (AB2IN_LEFT)</li>
                                            <li>Motor Driver Right: Pin 16 (AB1IN_RIGHT), Pin 15 (AB2IN_RIGHT)</li>
                                            <li>Separate power supplies for Artemis and motor drivers ensure smooth operation.</li>
                                        </ul>
                                        <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/artemis_pin.png" alt="...">
                                        <p>
                                            A diagram of the wiring layout is provided below:
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/motor_drive_connection.png" alt="...">
                                        <br>

                                        <h5>Lab Tasks</h5>
                                        <p>
                                            <strong>i. Motor Driver Tests</strong>
                                            <p>
                                            To test a single motor driver, the Artemis is connected to the motor driver, with the motor driver powered using an external power supply set to 3.7V. The Artemis is powered through USB
                                        </p></p>
                                        <p>
                                            Power Supply Settings: 
                                            <ul>
                                                <li>Motor Driver VIN & GND: Connected to 3.7V power supply.</li>
                                                <li>Artemis: Powered via USB-C.</li>
                                                <li>Oscilloscope Measurements: Confirm PWM signal output.</li>
                                            </ul>
                                        </p>
                                        <div class="d-flex justify-content-center mt-2">
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/power_supply.png" alt="...">
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/power_supply_connection.png" alt="...">
                                        </div>
                                        <p>
                                            With the setup complete, the motor control pin on the Artemis can be initialized using the following commands: analogWrite(AB1IN_LEFT, 63); analogWrite(AB2IN_LEFT, 0);Here, AB1IN_LEFT and 
                                            AB2IN_LEFT are defined as pins 13 and A14, respectively. The value 63 corresponds to a 25% duty cycle, producing the oscilloscope waveform shown on the left. Later, the duty cycle is increased 
                                            to 75% by setting the value to 127. Before testing with the oscilloscope, I soldered a wire to the DC motor, which introduced a slight distortion in the waveform. This distortion, likely caused 
                                            by the motor's motion, prevented the signal from appearing completely flat when high.
                                        </p>
                                        <p>
                                            Code for One Motor Testing
                                        </p>
                                        <pre>
    #define AB1IN_LEFT 3
    #define AB2IN_LEFT 14
    #define dc 60
    void setup() {
        pinMode(AB1IN_LEFT, OUTPUT);
        pinMode(AB2IN_LEFT, OUTPUT);
    }
    void loop() {
        analogWrite(AB1IN_LEFT, dc);
        analogWrite(AB2IN_LEFT, 0);
    }
                                        </pre>
                                        <div class="d-flex justify-content-center mt-2">
                                            <img class="img-fluid rounded w-50 mx-2" src="assets/img/portfolio/waveform1.jpg" alt="...">
                                            <img class="img-fluid rounded w-50 mx-2" src="assets/img/portfolio/waveform2.jpg" alt="...">
                                        </div>
                                        <br>
                                        <p>
                                            Therefore, one motor spinning is shown in this video below:
                                        </p>
                                        <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                            <source src="assets/img/portfolio/wheel_one_side_turning.mp4" type="video/mp4">
                                        </video>
                                        <br>
                                        <br>

                                        <p>
                                            <strong>ii. Wheels Spinning Test</strong>
                                            <p>
                                                After confirming PWM signal generation, the next step was running a motor with the power supply.
                                            </p>
                                        </p>
                                        <p>
                                            Code for Two Motor Testing
                                        </p>
                                        <pre>
    #define AB1IN_LEFT 3
    #define AB2IN_LEFT 14
    #define AB1IN_RIGHT 16
    #define AB2IN_RIGHT 15
    #define dc 60
    void setup() {
        pinMode(AB1IN_LEFT, OUTPUT);
        pinMode(AB2IN_LEFT, OUTPUT);
        pinMode(AB1IN_RIGHT, OUTPUT);
        pinMode(AB2IN_RIGHT, OUTPUT);
    }
    void forward() {
        analogWrite(AB1IN_LEFT, dc);
        analogWrite(AB2IN_LEFT, 0);
        analogWrite(AB1IN_RIGHT, dc);
        analogWrite(AB2IN_RIGHT, 0);
    }
    void loop() {
        forward();
        delay(2000);
    }
                                        </pre>
                                        <p>
                                            Hence, we can get two motors working properly.
                                        </p>
                                        <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                            <source src="assets/img/portfolio/power_supply_power_wheelturn.mp4" type="video/mp4">
                                        </video>
                                        <br>
                                        <br>

                                        <p>
                                            <strong>iii. Connection Diagram</strong>
                                            <p>        
                                                After testing the motor drivers and power supply, we finalized the connection diagram, as shown in the figure. The setup includes two TOF sensors—one positioned at the front and the other on the side—and an IMU placed on top of the battery box
                                                of the robot. Additionally, I used glue gun to organize and secure the cables and componenet for a cleaner and stable arrangement.
                                            </p>
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/car_latyout.JPG" alt="...">
                                        <br>

                                        <p>
                                            <strong>iv. Two Motors Powered by Batteries</strong>
                                            <p>        
                                                With all components assembled and soldered, the motors were powered by the two 3.7V batteries. A video demonstrating proper operation is provided.
                                            </p>
                                        </p>
                                        <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                            <source src="assets/img/portfolio/battery_power_wheelturn.mp4" type="video/mp4">
                                        </video>
                                        <br>
                                        <br>

                                        <p>
                                            <strong>v. Lower Limit PWM Value</strong>
                                            <p>
                                            The lowest PWM value required to move the car was determined to be 26.                                            
                                            </p>
                                        </p>
                                        <pre>
    #define AB1IN_LEFT 3
    #define AB2IN_LEFT 14
    #define AB1IN_RIGHT 16
    #define AB2IN_RIGHT 15
    #define dc 26
    void setup() {
        pinMode(AB1IN_LEFT, OUTPUT);
        pinMode(AB2IN_LEFT, OUTPUT);
        pinMode(AB1IN_RIGHT, OUTPUT);
        pinMode(AB2IN_RIGHT, OUTPUT);
    }
    void forward() {
        analogWrite(AB1IN_LEFT, dc);
        analogWrite(AB2IN_LEFT, 0);
        analogWrite(AB1IN_RIGHT, dc);
        analogWrite(AB2IN_RIGHT, 0);
    }
    void loop() {
        forward();
        delay(2000);
    }
                                        </pre>
                                        <p>
                                            This video demonstrate the motion of the robot with lower PWM value.
                                        </p>
                                        <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                            <source src="assets/img/portfolio/slow_pwm_car.mp4" type="video/mp4">
                                        </video>
                                        <br>
                                        <br>

                                        <p>
                                            <strong>vi. Calibration for Straight-Line Motion</strong>
                                            <p>
                                                We observed that the two are turning at a different rate, it is because the pressure applied by the upper level motor to the lower level is cause the wheel axial to have friction contact with the car frame, causing lower turning speed. 
                                                The upper level motor gear box is also pressing the lower level motor wires connection causing inefficient power transimission. Due to variations in motor performance, we added a calibration factor of 1.5 was applied on the right motor 
                                                to ensure straight-line movement.
                                            </p>
                                        <pre>
    #define AB1IN_LEFT 3
    #define AB2IN_LEFT 14
    #define AB1IN_RIGHT 16
    #define AB2IN_RIGHT 15
    #define dc 60
    #define dc_dif 1.5
    void setup() {
        pinMode(AB1IN_LEFT, OUTPUT);
        pinMode(AB2IN_LEFT, OUTPUT);
        pinMode(AB1IN_RIGHT, OUTPUT);
        pinMode(AB2IN_RIGHT, OUTPUT);
    }
    void forward() {
        analogWrite(AB1IN_LEFT, dc);
        analogWrite(AB2IN_LEFT, 0);
        analogWrite(AB1IN_RIGHT, dc*dc_dif);
        analogWrite(AB2IN_RIGHT, 0);
    }
    void loop() {
        forward();
        delay(2000);
    }
                                        </pre>
                                        <br>
                                        <p>
                                            It can be concluded in the video that the robot can move almost in a straight line with the implementation of the calibration factor.
                                        </p>
                                        <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                            <source src="assets/img/portfolio/car_move_straight.mp4" type="video/mp4">
                                        </video>
                                        <br>
                                        <br>

                                        <p>
                                            <strong>vii. Open Loop Control</strong>
                                            <p>
                                                A sequence of movement commands was implemented. The car will move forward for 2 sec, next turn left for 2 second, then move backward for 2 second, and finally turn right for 2 second.
                                            </p>
                                        <pre>
    #define AB1IN_LEFT 3
    #define AB2IN_LEFT 14
    #define AB1IN_RIGHT 16
    #define AB2IN_RIGHT 15
    #define dc 60
    #define dc_dif 1.5
    void setup() {
        pinMode(AB1IN_LEFT, OUTPUT);
        pinMode(AB2IN_LEFT, OUTPUT);
        pinMode(AB1IN_RIGHT, OUTPUT);
        pinMode(AB2IN_RIGHT, OUTPUT);
    }
    void backward() {
        analogWrite(AB1IN_LEFT, dc);
        analogWrite(AB2IN_LEFT, 0);
        analogWrite(AB1IN_RIGHT, dc*dc_dif);
        analogWrite(AB2IN_RIGHT, 0);
    }
    void forward() {
        analogWrite(AB1IN_LEFT, 0);
        analogWrite(AB2IN_LEFT, dc);
        analogWrite(AB1IN_RIGHT, 0);
        analogWrite(AB2IN_RIGHT, dc*dc_dif);
    }
    void turn_left(int speed) {
        analogWrite(AB1IN_LEFT, 0);
        analogWrite(AB2IN_LEFT, 0);
        analogWrite(AB1IN_RIGHT, 0);
        analogWrite(AB2IN_RIGHT, dc*dc_dif*speed);
    }
    void turn_left(int speed) {
        analogWrite(AB1IN_LEFT, 0);
        analogWrite(AB2IN_LEFT, dc*speed);
        analogWrite(AB1IN_RIGHT, 0);
        analogWrite(AB2IN_RIGHT, 0);
    }
    void pause() {
        analogWrite(AB1IN_LEFT, 0);
        analogWrite(AB2IN_LEFT, 0);
        analogWrite(AB1IN_RIGHT, 0);
        analogWrite(AB2IN_RIGHT, 0);
    }
    void loop() {
        pasue();
        delay(2000);
        forward();
        delay(2000);
        pasue();
        delay(2000);
        turn_left(2);
        delay(2000);
        backward();
        delay(2000);
        turn_right(2);
        delay(2000);
        pause();
        delay(2000);
    }
                                        </pre>
                                        <p>
                                            Therefore, we get the robot motion which is demonstrated in the following video.
                                        </p>
                                        <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                            <source src="assets/img/portfolio/car_open_loop.mp4" type="video/mp4">
                                        </video>
                                        <br>
                                        <br>
                                        <br>

                                        <H5>Conclusion</H5>
                                        <P>
                                            This lab introduced the principles of motor control using PWM signals. The successful execution of open-loop commands demonstrated the ability to control movement through pre-programmed 
                                            instructions. Challenges included noise interference from motor operation and calibration for straight-line motion. Future work could involve implementing closed-loop feedback for improved 
                                            accuracy and efficiency.
                                        </P>
                                        <br>

                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio Modal 5-->
        <div class="portfolio-modal modal fade" id="portfolioModal5" tabindex="-1" aria-labelledby="portfolioModal5" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 5<h3 class="text-warning">Linear PID control and Linear interpolation</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5 w-50" src="assets/img/portfolio/pid.png" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Introduction</h5>
                                        <p>
                                            In this lab, a PID control system was implemented to regulate the robot’s movement based on <strong>Time-of-Flight (ToF) sensor</strong>    
                                            data. The primary objective was to control the robot’s approach towards a wall and stop precisely <strong>1 ft (304mm) away</strong>, while minimizing overshoot and accounting for changing conditions such as varying start distances and floor surfaces.
                                            <p>To achieve this, <strong>Bluetooth</strong> Low Energy (BLE) communication was used to send and receive debugging data, allowing real-time analysis of the PID parameters and system performance.</p>
                                        </p>
                                        <br>

                                        <h5>Materials</h5>
                                        <ul>
                                            <li>1 x Assembled Robot (Artemis board, IMU, ToF sensors, motor drivers)</li>
                                            <li>1 x USB cable</li>
                                            <li>2 x Li-Ion 3.7V batteries</li>
                                            <li>2 x 4m ToF sensors (VL53L1X)</li>
                                            <li>1 x 9DOF IMU sensor</li>
                                        </ul>
                                        <br>

                                        <h5>BLE Communication Setup</h5>
                                        <p>
                                            The system was designed to operate via BLE commands, allowing remote tuning of Kp, Ki, and Kd values. The following approach was implemented:
                                        </p>
                                        <p>
                                            1. BLE Characteristics were defined for sending and receiving data:
                                        </p>
                                        <ul>
                                            <li><code>rx_characteristic_string</code>: Receives PID control commands.</li>
                                            <li><code>tx_characteristic_float</code>: Sends sensor readings.</li>
                                            <li><code>tx_characteristic_string</code>: Sends debugging messages.</li>
                                        </ul>
                                        <p>
                                            PID Control Activation via BLE: The PID control loop starts upon receiving the following command format:
                                        </p>
                                        <p><code>
                                            ble.send_command(CMD.PID_POSITION_CONTROL, "maxTime|Kp|Ki|Kd|TargetDistance")
                                        </code></p>
                                        <p>
                                            For data Logging & Transmission, data from the ToF sensor, motor control inputs, and pid input were stored in 2D arrays, and it was transmitted to the computer for visualization after execution.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/pid_BLE.png" alt="...">
                                        <br>

                                        <h5>Lab Tasks</h5>
                                        <p>
                                            <p>
                                                The PID control equation implemented was:
                                            </p>
                                        </p>
                                        <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/pid_equation.png" alt="...">
                                        <p>
                                            Where: 
                                            <ul>
                                                <li>e: Error (distance from target).</li>
                                                <li>Kp: Proportional gain.</li>
                                                <li>Ki: Integral gain (accumulates past error).</li>
                                                <li>Kd: Derivative gain (anticipates future error).</li>
                                            </ul>
                                            The proportional gain, Kp, adjusts the control response based on the current error, which is the difference between the desired and actual values. A higher Kp increases the corrective action but may also cause instability or overshooting. 
                                            <br>
                                            The integral gain, Ki, accounts for accumulated past errors, enhancing the corrective action when errors persist over time. However, an excessive Ki can lead to overshooting or oscillations due to the slow decay of accumulated errors. 
                                            <br>
                                            The derivative gain, Kd, predicts future errors by analyzing the rate of change of the error. It acts as a damping mechanism to reduce overshooting and regulate the speed of corrections, though it can also amplify noise.
                                            <br>

                                            Implementation in Code:
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/pid_speed_code.png" alt="...">
                                        <br>

                                        <p>
                                            <strong>Range&Sampling Discussion</strong>
                                            <p>
                                                Similar to my implementation in Lab3, the loop constantly check the ToF sensor data readiness and it only stores new data when the sensor is ready, stores old data when sensor not ready. This ensures a high loop rate without waiting for the sensor that has slow data collection rate.
                                                The sensor releases new data evey 40 ms. While the PID control waiting for new sensor data, it will use the old data to compute for motor input at a really high rate about every 12 ms.
                                            </p>
                                        </p>
                                        <p>
                                            Using the similar array storing and transimission method implemented in Lab3, which is have a arrays size 2500 row, storing new data every row, starting sending data before overwrite with new data if the array is full, and send data after data collection if array is not full.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/pid_data_store_1.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/pid_data_store_2.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/pid_data_store_3.png" alt="...">
                                        <br>
                                        <br>

                                        <h5>PID Tuning</h5>
                                        <p>
                                            At first, I used Kp=0.05, Ki=0.05, Kd=1, which led the robot to bump into the wall with a close initial displacement about 2 meter. With a very short distance for acceleration, the robot still run pretty fast and cannot stop itself from bumping into the wall, meaning that the Kp is too high 
                                            for high acceleration and Kd is too small for slowing down too late. The robot also didn't backup after hit the wall, meaning the Ki is too small. Here is the ToF vs Time plot with Kp=0.05, Ki=0.05, Kd=1:
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_.05_.05_1.png" alt="...">
                                        <p>
                                            To slow down the robot at an eariler and have more backup when too close to obstacle, I increased the Ki and Kd to 0.13 and 1.5, while decreasing the Kp to 0.03. Here is the ToF vs Time plot: 
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_.03_.13_1.png" alt="...">
                                        <p>
                                            The updated PID constant results in a slow acceleration but still bump into the wall cause by the late deceleration, so I increase the Kd to 5, 10, 20, 50, and 100, which shows a trend of slowing down at an eariler time with bigger gap between the obstacle and robot, but it still got too close when initial displacement increases.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_.03_.13_5.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_.03_.13_10.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_.03_.13_20.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_.03_.13_50.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_.03_.13_100_far.png" alt="...">
                                        <br>
                                        <p>
                                            After many rounds of robot PID control tests with different K constant, I conclude that when placing robot at initial displacement of 2m, Kp=0.02, Ki=0.13, Kd=20 is good enough due to small distance for acceleration:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_.03_.13_20_close.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/speed_.03_.13_20.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/k_.03_.13_20.png" alt="...">
                                            <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                                <source src="assets/img/portfolio/pid_.03_.13_20_2.mp4" type="video/mp4">
                                            </video>
                                            <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                                <source src="assets/img/portfolio/pid_.03_.13_20_1.mp4" type="video/mp4">
                                            </video>
                                            <br>
                                            However, when the robot initial placement is around or greater than 3 meter, the Kd of 20 is not enough. The testing result shows a Kp=0.02, Ki=0.13, Kd=200 with the max velocity of 60 will led the robot stop at exactly 1 ft when initial place about 4 and 5 meter away:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/tof_.02_.13_200.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/speed_.02_.13_200.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/k_.02_.13_200.png" alt="...">
                                            <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                                <source src="assets/img/portfolio/pid_.03_.13_100_1.mp4" type="video/mp4">
                                            </video>
                                            <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                                <source src="assets/img/portfolio/pid_.03_.13_100_2.mp4" type="video/mp4">
                                            </video>
                                            <br>
                                        </p>
                                        <br>


                                        <H5>Conclusion</H5>
                                        <P>
                                            This lab demonstrated <strong>real-time PID control</strong> for robotic motion using BLE and ToF sensors. The PID controller successfully adjusted motor speed based on ToF distance. Key takeaways include:
                                            <ul>
                                                <li>Sensor sampling rate affects PID loop performance.</li>
                                                <li>Fine-tuning of PID gains:
                                                    <ul>
                                                        <li>Lower Kp → slower response to reduce acceleration rate to have better control for PID calculation.</li>
                                                        <li>Higher Ki → More error accumulation for robot backup correction when too close to obstacle.</li>
                                                        <li>Higher Kd → More damping to create better stability for eariler deceleration.</li>
                                                    </ul>
                                                </li>
                                            </ul>
                                            Future improvements include Kalman filtering (Lab 7) for more accurate sensor fusion.
                                        </P>
                                        <br>

                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio Modal 6-->
        <div class="portfolio-modal modal fade" id="portfolioModal6" tabindex="-1" aria-labelledby="portfolioModal6" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 6<h3 class="text-warning">Orientation Control</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5 w-50" src="assets/img/portfolio/orien_pic.webp" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Introduction</h5>
                                        <p>
                                            This lab focuses on implementing a PID controller to control the yaw orientation of a robot using an IMU. Unlike Lab 5, which controlled distance using TOF sensors, this lab employs a gyroscope to estimate the robot's orientation. The goal is to achieve stable stationary orientation control through a differential drive system. Challenges include sensor bias, integration drift, derivative kick, and Bluetooth command handling</p>
                                        </p>
                                        <br>

                                        <h5>Materials</h5>
                                        <ul>
                                            <li>1 x Assembled Robot (Artemis board, IMU, ToF sensors, motor drivers)</li>
                                            <li>1 x USB cable</li>
                                            <li>2 x Li-Ion 3.7V batteries</li>
                                            <li>2 x 4m ToF sensors (VL53L1X)</li>
                                            <li>1 x 9DOF IMU sensor</li>
                                            <li>2 x Dual motor driver</li>
                                        </ul>
                                        <br>

                                        <h5>BLE Communication Setup</h5>
                                        <p>
                                            To allow real-time tuning and monitoring, the PID controller was integrated with the Bluetooth communication system implemented in Lab5, allowing remote tuning of Kp, Ki, and Kd values. The following approach was implemented:
                                        </p>
                                        <p>
                                            1. BLE Characteristics were defined for sending and receiving data:
                                        </p>
                                        <ul>
                                            <li><code>rx_characteristic_string</code>: Receives PID control commands.</li>
                                            <li><code>tx_characteristic_float</code>: Sends sensor readings.</li>
                                            <li><code>tx_characteristic_string</code>: Sends debugging messages.</li>
                                        </ul>
                                        <p>
                                            PID Control Activation via BLE: The PID control loop starts upon receiving the following command format:
                                        </p>
                                        <p><code>
                                            ble.send_command(CMD.PID_OIREN_CONTROL, "maxTime|Kp|Ki|Kd|SetPoint")
                                        </code></p>
                                        <p>
                                            Similar to my implementation in Lab3 and Lab5, the loop constantly check the IMU sensor data readiness and it only stores new data when the sensor is ready, stores old data when sensor not ready. This ensures a high loop rate without waiting for the sensor that has slow data collection rate.
                                            The sensor releases new data evey 40 ms. 
                                        </p>
                                        <p>
                                            While the PID control waiting for new sensor data, it will use the old data to compute for motor input at a really high rate about every 12 ms.
                                            Using the similar array storing and transimission method implemented in Lab3, which is have a arrays size 2500 row, storing new data every row, starting sending data before overwrite with new data if the array is full, and send data after data collection if array is not full.
                                        </p>
                                        <p>C++ script for sending data from Artemis: </p>
                                        <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_data_record.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_data_store.png" alt="...">
                                        <p>Python script for receiving data from Artemis: </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_BLE.png" alt="...">
                                        <br>

                                        <h5>IMU Bias Compensation</h5>
                                        <p>To ensure the IMU's accuracy over time, we should determine the gyroscope's bias parameter during the Artemis setup. After running the setup script, we obtained the gyroscope bias range from 0.6 to 0.9.
                                        Implementation in code:</p>
                                        <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_bias.png" alt="...">
                                        <br>
                                        <br>

                                        <h5>Lab Tasks</h5>
                                        <p>
                                            <strong>PID Control Strategy</strong>
                                            <p>
                                                To stabilize orientation, a PID controller was implemented with tunable proportional, integral, and derivative gains:
                                            </p>
                                            <ul>
                                                <li>e: Error (distance from target).</li>
                                                <li>Proportional Gain (Kp): Reacts to the current error, adjusting motor speeds accordingly</li>
                                                <li>Integral Gain (Ki): Compensates for accumulated past errors, reducing steady-state error.</li>
                                                <li>Derivative Gain (Kd): Predicts future errors by evaluating the rate of error change, damping oscillations.</li>
                                            </ul>
                                        </p>
                                        <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/pid_equation.png" alt="...">
                                        <p>
                                            Before the PID orientation implementation, I first tested the PWM needed for robot turning cw and ccw, and obtain a minimum turning PWM signal of 100 with right motor requires 60% more PWM signal in order to turn with same speed as left motor.
                                            Implementation in Code below:
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/pid_pwm_orien.png" alt="...">
                                        <br>
                                        <br>

                                        <h5>PID Tuning</h5>
                                        <p>
                                            At first, I used Kp=10, Ki=0.1, Kd=0, which led the robot to spin none stop, I first conclude that it is because the robot kp is too high leading high turning speed that misled the PID judgment:
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/no_10_0.1_0_pwm.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/no_10_0.1_0_k.png" alt="...">
                                        <p>
                                            With Kp=0.5, Ki=0.1, Kd=0, the robot still spinning none stop, showing the robot has an rapidly accumulating integrated err, which overcontrol kp and ki. 
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/no_0.5_.1_0_pwm.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/no_0.5_0.1_0_k.png" alt="...">
                                        <p>
                                            To resolve that, I added in a limit for the maximum integrated err value, to keep the robot under control.
                                        </p>
                                        <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_wind_up.png" alt="...">
                                        <p>
                                           After adding the wind-up protection of 500 max integrated err, the robot was able to adjust its angle to the desired set point, but it will tweak with big movement around the set point, indicating the need to add in an Kd for stablization when reaching the setpoint.  
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_5_0.05_0_pwm.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_5_0.05_0_k.png" alt="...">
                                        <p>
                                            By keeping the Kp and Ki the same, adding Kd of 10, 50, and 100, the plot displays the reduction of tweaking around set point, but still cannot remove fully even with big Kd value. 
                                        </p>
                                        <p>Kp=5, Ki=0.05, Kd=10</p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_5_0.05_10_pwm.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_5_0.05_10_k.png" alt="...">
                                        <p>Kp=5, Ki=0.05, Kd=50</p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_5_0.05_50_pwm.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_5_0.05_50_k.png" alt="...">
                                        <p>Kp=5, Ki=0.05, Kd=100</p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_5_0.05_100_pwm.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_5_0.05_100_k.png" alt="...">
                                        <p>
                                            To further remove the tweaking around set point, I decreased Kp and Ki to reduce the effect of current err and integrated err on the PWM, which helps the robot to gain better control and stablilty around the desired angle.
                                        </p>
                                        <p>Kp=2, Ki=0.005, Kd=100</p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_2_0.005_100_pwm.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_2_0.005_100_k.png" alt="...">
                                        <p>
                                            To allow the robot to accurately adjust its angle (ex. -90 to 90 degree), I further increased the Kd value to capture the precise PWM control by actively checking its err derivate to prevent turning overshoot.
                                        </p>
                                        <p>Kp=2, Ki=0.005, Kd=200</p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_2_0.005_200_pwm.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_2_0.005_200_angle.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_2_0.005_200_k.png" alt="...">
                                        <br>
                                         
                                        <p>
                                            After many rounds of robot PID control tests with different K constant, I conclude that the best PID constant for orientation control with full battery state of charge is Kp=2, Ki=0.13, Kd=200:
                                            However, when the robot battery state of charge being relatively low, the Kp need to increases to 5 in order for the robot to turn effectionly. Here is the video of robot adjusting its angle to desired set point when being move by force externally:
                                            <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                                <source src="assets/img/portfolio/orien_adjust.mp4" type="video/mp4">
                                            </video>
                                        </p>
                                        <br>
                                        <p>
                                            I also wrote an python open loop script for the robot orientation control, telling it to turn to differe angle multiple time. The robot reacts with pretty accurate orientation control. Implemented in code:
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/orien_control_loop.png" alt="...">
                                        <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                            <source src="assets/img/portfolio/orien_loop.mp4" type="video/mp4">
                                        </video>
                                        <br>
                                        <br>

                                        <h5>Orien PID Discussion</h5>
                                        <p>
                                            <ul>
                                            <li>Gyroscope drift was reduced by calculating bias at initialization.</li>
                                            <li>Higher Kp led to faster corrections but increased oscillations.</li>
                                            <li>Ki helped eliminate steady-state error but caused overshoot when too high.</li>
                                            <li>Kd improved damping but required fine-tuning to avoid instability.</li>
                                            </ul>
                                        </p>
                                        <br>

                                        <H5>Conclusion</H5>
                                        <P>
                                            This lab successfully demonstrated stationary orientation control using a PID controller. Through parameter tuning, wind-up protection, and Bluetooth integration, a robust control system was implemented. The results highlight the importance of PID balance for achieving stability, accuracy, and responsiveness in robotic orientation control.
                                        </P>
                                        <br>

                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio Modal 7-->
        <div class="portfolio-modal modal fade" id="portfolioModal7" tabindex="-1" aria-labelledby="portfolioModal7" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 7<h3 class="text-warning">Kalman Filter</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5 w-50" src="assets/img/portfolio/kf.png" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Introduction</h5>
                                        <p>
                                            The objective of this lab is to implement a Kalman Filter (KF) to enhance the performance of distance estimation using Time-of-Flight (ToF) sensors. By incorporating the KF, the robot should be able to approach the wall at high speeds while accurately estimating its position to either stop 1 ft away or turn within 2 ft. The Kalman Filter is first tested in Jupyter Notebook and then integrated into the Artemis robot.
                                        </p>
                                        <br>

                                        <h5>Materials</h5>
                                        <ul>
                                            <li>1 x Assembled Robot (Artemis board, IMU, ToF sensors, motor drivers)</li>
                                            <li>1 x USB cable</li>
                                            <li>2 x Li-Ion 3.7V batteries</li>
                                            <li>2 x 4m ToF sensors (VL53L1X)</li>
                                            <li>1 x 9DOF IMU sensor</li>
                                            <li>2 x Dual motor driver</li>
                                            <li>1 x Crash-pillows (for testing safely)</li>
                                        </ul>
                                        <br>

                                        <h5>Lab Tasks</h5>
                                        <p>
                                            <strong>1. Estimate Drag and Momentum</strong>
                                            <p>
                                                To build the state-space model, the drag and momentum terms were estimated using a step response test. The robot was driven toward a wall at a constant PWM input while logging motor input values and ToF sensor readings. Using the data transmission function implemented for the PID control, I sent the collected ToF and time data through BLE. To produce a cleaner plot, I wrote function to filter the repeatitive ToF value and use the filtered value to calculate the robot velocity:
                                            </p>
                                            Data filtering:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_distance_code.png" alt="...">
                                            Velocity Calculation:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_speed_code.png" alt="...">
                                            Here is the resulting plot of ToF and Velocity vs time while the robot driving toward the wall:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_pwm.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_distance.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_speed.png" alt="...">
                                            With the data plot I obtained for ToF and Velocity vs time, I concluded that the Steady-State Speed is ~750 mm/s and the 90% Rise Time is ~1 s. Thus, the drag and momentum parameters can be initialized as below.
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_d_m.png" alt="...">
                                        </p>
                                        <br>

                                        <p>
                                            <strong>2. Initialize Kalman Filter (Python Implementation)</strong>
                                            <p>
                                            After collecting the data and initializing the parameters, the Kalman Filter (KF) can be configured accordingly. 
                                            The equations presented in the lecture slides define the Kalman Filter framework. To implement it, key parameters must be identified, including the state-space matrices A, B, and C, as well as the noise matrices σ_u and σ_z.
                                            State-Space Matrices
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_lecture.png" alt="...">
                                            The system state-space model is defined by matrices A, B, and C. Given the ToF sensor sampling rate is about 100ms determined in Lab3, so the system was discretized with ΔT=0.1s.
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_A_B_code.png" alt="...">
                                            Hence, the A and B matrix can be obtain:
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_A_B1.png" alt="...">
                                            Next, the C matrix can be determined as an m × n matrix, where n represents the dimensions of the state space, and m corresponds to the number of measured states. In this case, the C matrix is [-1, 0], as the distance is measured from negative to zero as the robot approaches the wall. Additionally, the state vector X should be initialized as the first ToF data.
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_C_code.png" alt="...">
                                            Furthermore, the process noise covariance (Σu) and sensor noise covariance (Σz) were initialized with rough estimates and later fine-tuned:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_sigma_code.png" alt="...">
                                            <img class="img-fluid rounded w-25 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_sigma.png" alt="...">
                                            Currently, the variance values are only rough estimates. They will be tested and adjusted later to determine the optimal values.
                                            </p>
                                        </p>
                                        <br>

                                        <p>
                                            <strong>3. Kalman Filter Function in Jupyter</strong>
                                            <p>
                                            The Kalman Filter function python implementation is given by the lab guide. After copying the code over to jupyter lab, I created an array, kf_data, to store the KF filter result.
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_function.png" alt="...">
                                            With the initial state and covariance matrix declared as first ToF data and [5^2 0; 0 5^2], the collected ToF data was processed through the Kalman Filter to validate its performance and then plotted.  The Kalman Filter output was compared to raw ToF measurements:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_5_100_20.png" alt="...">
                                            The first plot shows that a Σz of 400 makes the KF trusts sensor data less, leading it to lags more behind the actual sensor data and overly dependent on the process model. After tuning the Σz down to 100, the KF result fit more closely:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_5_100_10.png" alt="...">
                                            The second plot shows that a low Σu of [1000 0; 0 1000] makes the KF too confident, leading some inaccurate prediction. The gap between the KF and real ToF data is much much smaller after increasing Σu to [4000 0; 0 4000]:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_5_200_10.png" alt="...">
                                            The third plot still isn't perfect, by further increasing the Σu to [4000 0; 0 4000], we obtain the most fit KF result:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_5_400_10.png" alt="...">
                                            Initially, the filter was too sensitive to sensor noise, requiring parameter adjustments. By tuning covariance matrices (Σu,Σz), a smoother estimate was obtained.
                                            </p>
                                        </p>
                                        <br>

                                        <p>
                                            <strong>4. Kalman filter at a faster frequency</strong>
                                            To increase the Kalman Filter's operating frequency, I modified the sampling rate. This results in a time step of 20ms when ΔT = 0.1/5. Between readings, the prediction step is used to estimate the car's state. While this step resembles the linear extrapolation method from Lab 6, it is specifically part of the Kalman Filter process.
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_fast1.png" alt="...">
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_fast2.png" alt="...">
                                            Using the new delta_T of 0.02, I obtain the figure as below, which illustrates the diagram with decreasing delta_T:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_inter_mea.png" alt="...">
                                        </p>
                                        <br>

                                        <p>
                                            <strong>5. Implement Kalman Filter on Artemis</strong>
                                            The Kalman Filter was then implemented in C++ on the Artemis microcontroller. I imported an new library <code>BasicLinearAlgebra.h</code> for matrix manipulation. 
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_library.png" alt="...">
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_matrix_declare.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_function_c.png" alt="...">
                                            By running the PID ToF control using KF on the robot over BLE, we obtained 2 plot of robot distance with/without KF:
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_compare1.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/kf_compare3.png" alt="...">
                                            Both plot result shows the robot position itself closer to the targer distance (300mm) away from wall using KF, while the robot overshoot itself passed the target distance. This indicates the robot has a better control when equipped with KF in PID control. 
                                        </p>
                                        <br>

                                        <h5>Discussion</h5>
                                        <p>
                                            The Kalman Filter significantly improved distance estimation by reducing noise and enabling the robot to react more accurately. Key insights include:
                                            <ul>
                                                <li>Proper tuning of process and measurement noise matrices was essential for stability.</li>
                                                <li>Running the Kalman Filter at a higher frequency improved performance.</li>
                                                <li>The filter successfully complemented the PID controller, allowing the robot to approach the wall safely at higher speeds.</li>
                                                <li>Future work involves optimizing execution speed further and integrating the Kalman Filter with Lab 8's tasks.</li>
                                            </ul>
                                        </p>
                                        <br>

                                        <h5>Conclusion</h5>
                                        <p>
                                            The implementation of the Kalman Filter enabled faster and more reliable state estimation, allowing the robot to navigate with improved precision. Successful integration on the Artemis demonstrated its effectiveness in real-time control applications.
                                        </p>

                                        

                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio Modal 8-->
        <div class="portfolio-modal modal fade" id="portfolioModal8" tabindex="-1" aria-labelledby="portfolioModal8" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 8<h3 class="text-warning">Stunts! – Flip Challenge</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5 w-50" src="assets/img/portfolio/stunt.png" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Introduction</h5>
                                        <p>
                                            The objective of this lab is to perform an autonomous robot stunt using the hardware and software framework built over the course of the semester. Specifically, we aim to execute a forward flip maneuver using speed and controlled braking, leveraging the onboard Time-of-Flight (ToF) sensor, motor controls, and Kalman Filter (KF) for fast and accurate distance estimation.
                                        </p>
                                        <br>

                                        <h5>Materials</h5>
                                        <ul>
                                            <li>1 x Assembled Robot (Artemis board, IMU, ToF sensors, motor drivers)</li>
                                            <li>1 x USB cable</li>
                                            <li>2 x Li-Ion 3.7V batteries</li>
                                            <li>2 x 4m ToF sensors (VL53L1X)</li>
                                            <li>1 x 9DOF IMU sensor</li>
                                            <li>2 x Dual motor driver</li>
                                            <li>1 x Crash-pillows (for testing safely)</li>
                                        </ul>
                                        <br>

                                        <h5>Lab Tasks</h5>
                                        <p>
                                            <strong>1. Accurate ToF data with KF</strong>
                                            <p>
                                                For the flip stunt, I used the Kalman Filter developed in Lab 7 to accurately estimate the distance to the wall using the ToF sensor. This estimation runs at a higher effective frequency by combining sensor input and prediction modeling, which is critical for fast maneuvers where ToF readings alone may be too slow or noisy.
                                            </p>
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/stunt_kf.png" alt="...">
                                        </p>
                                        <br>
                                        <p>
                                            <strong>2. Stunt States and Logic</strong>
                                            <p>
                                                I implemented a function kf_position_tof() that controls the stunt behavior based on the robot's distance from the wall. The robot proceeds through three states:
                                                <ul>
                                                    <li><strong>State 1 – Forward Acceleration:</strong> The robot starts from rest and accelerates forward at high speed. KF continuously estimates the position using motor input and ToF readings. Sometimes, the robot may fail to detect the distance correctly and continue moving indefinitely. To prevent this, I added a timeout condition that forces the robot to stop if it remains in this state for more than a couple second (input from jupyter lab). State 1 also record the start time of the flip for timing the flip.</li>
                                                    <li><strong>State 2 – Flip Trigger:</strong> Once within ±100mm of the target flip distance (~300m input from jupyter lab), the robot reverses motor direction at max speed to produce an abrupt deceleration. This generates a torque that causes the robot to pitch forward and flip. The time of reaching state 2 is also record to obtain time from start to flip.</li>
                                                    <li><strong>State 3 – Timed Backward Stop:</strong> After flipping, the robot drives backward for the time difference between start and flip before stopping, approximating a return to the starting region.</li>
                                                </ul>
                                            </p>
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/stunt_control.png" alt="...">
                                        </p>
                                        <p>
                                            <strong>3. Jupyter lab control</strong>
                                            <p>
                                                By using the similar BLE setup on the Artemis, I was able to control robot to perform stunt remotely using jupyterlab and BLE by sending command:
                                            </p>
                                            <img class="img-fluid rounded w-100 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/stunt_jupyter.png" alt="...">
                                        </p>
                                        <br>
                                        <p>
                                            <strong>3. Stunt recording</strong>
                                            <p>
                                                Below is three successful stunts of the robot flip and one really cool stunts the robot accidentally made:
                                                <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                                    <source src="assets/img/portfolio/stunt1.mp4" type="video/mp4">
                                                </video>
                                                <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                                    <source src="assets/img/portfolio/stunt2.mp4" type="video/mp4">
                                                </video>
                                                <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                                    <source src="assets/img/portfolio/stunt3.mp4" type="video/mp4">
                                                </video>
                                                <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                                    <source src="assets/img/portfolio/stunt_cool.mp4" type="video/mp4">
                                                </video>
                                            </p>
                                            <p>
                                                To collect flip data, I created a SENDING_FLIP_DATA case to transmit the recorded values from the Artemis to the laptop via the BLE channel. The motor commands indicate that the robot initiates the flip maneuver when it is within 1/2 second of the target wall distance.
                                                Using the same data transfer method as in previous labs, I sent arrays of ToF and PWM data from the Artemis to Jupyter Lab and plotted them over time.                                            
                                            </p>
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/stunt_plot1.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/stunt_plot2.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/stunt_plot3.png" alt="...">
                                            <p>
                                                From the graph, we can see that the robot began its stunt approximately 2 meters away from the wall and initiated the flip roughly 5–10 cm from the wall. After the flip, it moved backward. The ToF readings during the reverse motion are less accurate due to the sensor being mounted at a slight downward angle. This positioning caused the sensor to occasionally capture reflections from the ground instead of directly ahead. The sudden spikes in distance readings correspond to moments when the robot flips and the front wheels briefly lift off the ground, allowing the sensor to momentarily detect a farther range before settling back.
                                            </p>
                                        </p>
                                        <br>

                                        <h5>Conclusion</h5>
                                        <p>
                                            This lab successfully brought together core components—sensors, estimation, and control logic—to execute a fast and dramatic stunt. The forward flip was achieved using a combination of speed, precise triggering, and timed reversal. The KF-based localization greatly improved reliability, allowing consistent performance across multiple runs. While tuning the flip distance was tricky due to slight variation in sensor readings and sticky mat placement, once dialed in, the stunt proved repeatable and robust.
                                        </p>
                                        

                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio Modal 9-->
        <div class="portfolio-modal modal fade" id="portfolioModal9" tabindex="-1" aria-labelledby="portfolioModal9" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 9<h3 class="text-warning">Mapping</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5 w-50" src="assets/img/portfolio/map_global_wall.png" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Introduction</h5>
                                        <p>
                                            This lab involved mapping a room using a robot equipped with Time-of-Flight (ToF) sensors and an IMU. The robot performed in-place rotations at designated positions while collecting distance data. The quality of the map depended on how evenly spaced the ToF samples were during rotation. Three control methods were proposed: open loop control, PID on orientation, and PID on angular velocity. In this lab, we implemented PID orientation control to perform consistent angle-based rotations. After data collection, we merged the scans using trigonometric projection and visualized the map in both polar and global frames.
                                        </p>
                                        <br>

                                        <h5>Materials</h5>
                                        <ul>
                                            <li>1 x Assembled Robot (Artemis board, IMU, ToF sensors, motor drivers)</li>
                                            <li>1 x USB cable</li>
                                            <li>2 x Li-Ion 3.7V batteries</li>
                                            <li>2 x 4m ToF sensors (VL53L1X)</li>
                                            <li>1 x 9DOF IMU sensor</li>
                                            <li>2 x Dual motor driver</li>
                                        </ul>
                                        <br>

                                        <h5>Lab Tasks</h5>
                                        <p>
                                            <strong>PID orientation controll</strong>
                                            <p>
                                                To achieve accurate in-place rotation, we used our previously implemented <code>pid_orien_imu()</code> function, which adjusts motor PWM values based on the angular error between the robot's current orientation and a target angle. We initialized a mapping loop where the robot updated its target heading by a fixed angle every <code>accel_gap</code> milliseconds. The desired angular increment and rotation speed were transmitted from the laptop via BLE.
                                            </p>
                                            <p>PID control code:</p>
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/map_pid_control_code.png" alt="...">
                                            <p>ToF data collect code:</p>
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/map_data_collect_code.png" alt="...">
                                            <p>
                                                After careful tuning and testing of the PID control, we obtained the control command below. This rotated the robot in ~10° increments, each with a puase of ~0.4 second, collecting ~36 samples per scan position. The controller ensured consistent angular steps and eliminated drift that often affected open loop control. 
                                            </p>
                                            <img class="img-fluid rounded w-100 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/map_data_collect_command.png" alt="...">
                                            <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                                <source src="assets/img/portfolio/map_data_collect.mp4" type="video/mp4">
                                            </video>
                                            <br>
                                            
                                            <strong>Data transimission</strong>
                                            <p>
                                                After the rotation completed, the robot to send back time and distance data using the same method as previous lab, an 2D array to store the ToF sensor data and its timestamp and send the data through BLE when duration time ended or array is full(continue collects data and send at the same time to ensure storage and data continuity).
                                            </p>
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/map_data_send.png" alt="...">
                                        </p>
                                        <br>

                                        <h5>Mapping</h5>
                                        <p>
                                            <strong>Polar Plot Generation</strong>
                                            <p>
                                                After collecting ToF dataset at 5 different location in the map, I obtained the following raw distance data plots:
                                            </p>
                                            <div class="row text-center">
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_flat_-3_-2.png" alt="...">
                                                </div>
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_flat_0_0.png" alt="...">
                                                </div>
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_flat_0_3.png" alt="...">
                                                </div>
                                            </div>
                                            <div class="row justify-content-center text-center">
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_flat_5_-3.png" alt="...">
                                                </div>
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_flat_5_3.png" alt="...">
                                                </div>
                                            </div>
                                            <p>
                                                Given the use of PID control, the data obtained was collected in relatively even spaced angular steps with the starting angle <code>theta</code>. The corresponding angle for each sample was calculated using np.linspace:
                                            </p>
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/map_polar_code.png" alt="...">
                                            <br>
                                            <p>
                                                This provided a clean polar representation of the space surrounding each scan location.
                                            </p>
                                            <div class="row text-center">
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_polar_-3_-2.png" alt="...">
                                                </div>
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_polar_0_0.png" alt="...">
                                                </div>
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_polar_0_3.png" alt="...">
                                                </div>
                                            </div>
                                            <div class="row justify-content-center text-center">
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_polar_5_-3.png" alt="...">
                                                </div>
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_polar_5_3.png" alt="...">
                                                </div>
                                            </div>
                                            <strong>Global Coordinate Mapping</strong>
                                            <p>
                                                With each scan taken at a known position <code>(x, y)</code> in the map frame, and each sample associated with a global direction theta, I computed the transformed coordinates:
                                            </p>
                                            <img class="img-fluid rounded w-50 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/map_matrix.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/map_global_code.png" alt="...">
                                            <p>
                                                Each scan dataset was plotted with a unique color. The data aligned well and traced walls and obstacles pretty accurately in close distance (within 0.5m). Minor offsets in far distance data between scans were likely due to small moved distance robot made while turning for 360 data sampling or different starting and ending robot position.
                                            </p>
                                            <div class="row text-center">
                                                <div class="col-md-4">
                                                  <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_global_-3-2.png" alt="...">
                                                </div>
                                                <div class="col-md-4">
                                                    <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_global_00.png" alt="...">
                                                </div>
                                                <div class="col-md-4">
                                                    <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_global_03.png" alt="...">
                                                </div>
                                            </div>
                                            <div class="row justify-content-center text-center">
                                                <div class="col-md-4">
                                                    <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_global_53.png" alt="...">
                                                </div>
                                                <div class="col-md-4">
                                                    <img class="img-fluid rounded mb-3" src="assets/img/portfolio/map_global_5-3.png" alt="...">
                                                </div>
                                            </div>
                                            <p>
                                                Due to time constrain, I was only able to collect data at locations [-3, -2], [0, 0], [0, 3], [5, 3], and [5, -3] twice. With no calibration, all those points can be plotted into the same diagram. We can see that the robot captured most of the mapping shape and obstacle wall accurately, with minor data point shifting and little to no outstander.
                                            </p>
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/map_global.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/map_wall_code.png" alt="...">
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/map_global_wall.png" alt="...">
                                            <p>
                                                I drew the estimated map based on the ToF mapping, which we can see alined accurately in some places while being place in between different data plotting results. This will most likely be resolved when I collect more data in different location repeatively in future. 
                                            </p>
                                        </p>
                                        <br>

                                        <h5>Conclusion</h5>
                                        <p>
                                            In this lab, I successfully implemented closed-loop PID orientation control to perform precise in-place rotations for room mapping using Time-of-Flight sensors and an IMU. The system rotated the robot in controlled ~10° increments with stable timing, producing evenly spaced angular samples for each 360° scan. By combining the collected distance data with known robot positions and orientations, I converted local ToF readings into global coordinates and generated both polar and Cartesian maps of the room.
                                        </p>
                                        <p>
                                            The results show that the robot was able to accurately capture surrounding walls and obstacles, particularly within short-range distances (~0.5 meters). While some minor misalignments and offset points appeared—especially in farther regions or between different scan locations—these were likely caused by small shifts in the robot’s position during rotation and slight inconsistencies in the initial heading.
                                        </p>
                                        <p>
                                            Despite limited time and only two scans per location, the compiled map already closely resembles the actual map layout. With additional repeated measurements and calibration, these remaining discrepancies can be minimized. Overall, this lab demonstrated the effectiveness of closed-loop orientation control for reliable spatial data acquisition and laid the groundwork for future localization and navigation tasks.
                                        </p>
                                        <br>

                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio Modal 10-->
        <div class="portfolio-modal modal fade" id="portfolioModal10" tabindex="-1" aria-labelledby="portfolioModal10" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 10<h3 class="text-warning">Grid Localization using Bayes Filter (Simulation)</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5 w-50" src="assets/img/portfolio/local_traj3.png" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Introduction</h5>
                                        <p>
                                            The purpose of this lab is to implement a <strong>grid-based robot localization</strong> system using the <strong>Bayes Filter</strong>. In this system, the robot navigates within a simulated environment, and its location 
                                            is estimated probabilistically over time by maintaining a belief distribution across a discretized 3D grid of possible states (x, y, θ). This belief is updated in two stages — using <strong>odometry data</strong> during the 
                                            <strong>prediction step</strong>, and using <strong>sensor readings</strong> during the <strong>update step</strong>.
                                        </p>
                                        <p>
                                            The goal is to localize the robot accurately despite noisy motion and sensor data. This lab demonstrates how motion estimation and sensor correction can be integrated in a Bayesian framework to achieve robust localization.
                                        </p>
                                        <br>

                                        <h5>Prelab</h5>
                                        <p>
                                            <strong>Background</strong>
                                            The Bayes Filter maintains a probabilistic estimate of the robot's location. At each time step, it goes through:
                                            <ul>
                                                <li><strong>Prediction Phase:</strong> Integrates odometry (control input) to estimate the robot’s movement.</li>
                                                <li><strong>Update Phase:</strong> Refines the estimate using range measurements from onboard sensors.</li>
                                            </ul>
                                            This leads to two types of beliefs:
                                            <ul>
                                                <li><strong>Prior belief (bel_bar):</strong> After prediction</li>
                                                <li><strong>Posterior belief (bel):</strong> After sensor-based correction</li>
                                            </ul>
                                            The robot's environment is discretized into a 3D grid of poses (x, y, θ), with each cell representing a possible robot state. We implement five key components to realize the Bayes Filter:
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_bayes.png" alt="...">
                                        <br>

                                        <h5>Lab Tasks</h5>
                                        <p>
                                            <strong>Compute control</strong>
                                            <p>
                                                This function extracts control parameters from odometry: the first rotation (<code>rot1</code>) aligns the robot with its direction of motion, the translation (<code>trans</code>) measures the straight-line distance traveled, and the second rotation (<code>rot2</code>) aligns the robot with its new heading. It uses trigonometry and normalization to ensure angles are within [−180°,+180°]. These control values are used by the <strong>prediction step</strong> and <strong>odometry motion model</strong> to simulate and evaluate the robot’s motion under uncertainty.
                                            </p>
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_control.png" alt="...">

                                            <strong>Odometry Motion Model</strong>
                                            <p>
                                                This function models the <strong>uncertainty in robot motion</strong> by computing the likelihood that a given transition between poses is consistent with a control input. It uses the Gaussian distribution to score how likely the observed motion is, based on known noise parameters for rotation and translation. It plays a central role in the <strong>prediction step</strong>, determining how prior belief values propagate to future poses based on motion.
                                            </p>
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_odom.png" alt="...">

                                            <strong>Prediction Step</strong>
                                            <p>
                                                The prediction step estimates where the robot could be after applying a motion command, and produces the <strong>prior belief</strong> (<code>bel_bar</code>). It uses <strong>control data from odometry</strong> to determine how likely the robot is to have transitioned from any previous state (<code>px, py, pa</code>) to a new state (<code>cx, cy, ca</code>).
                                            </p>
                                            <p>
                                                To compute this,the algorithm:
                                            </p>
                                            <ul>
                                                <li>Extracts the motion command using <code>compute_control()</code>, which calculates the robot’s relative movement as a tuple (<code>rot1, trans, rot2</code>) based on odometry readings.</li>
                                                <li>Loops through all <strong>previous states</strong> with significant probability (filtering out very low values to improve efficiency).  </li>
                                                <li>For each possible <strong>current state</strong>, it computes the likelihood of that transition using the odometry motion model (<code>odom_motion_model()</code>), which applies a Gaussian probability based on the motion noise.</li>
                                                <li>It then updates the prior belief by multiplying the transition probability by the belief at the previous state and adding that contribution to the current state's prior.</li>
                                            </ul> 
                                            <p>
                                                Finally, it <strong>normalizes the belief grid</strong> <code>bel_bar</code> to ensure the total probability sums to 1. This prior is then used in the <strong>update step</strong>, where sensor data will refine the belief.
                                            </p>
                                            <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_pred.png" alt="...">

                                            <strong>Update Step</strong>
                                            <p>
                                                This function <strong>updates the belief</strong> <code>bel</code> using sensor measurements. In this implementation, I did not use a separate <code>sensor_model()</code> function. Instead, I chose to perform sensor <strong>likelihood calculations directly</strong> within the <code>update_step()</code> function, which makes the code more efficient and easier to follow. Each time the Bayes filter needs to evaluate the likelihood of a pose, it directly accesses the <strong>expected sensor data</strong> for that pose using <code>mapper.get_views(cx, cy, ca)</code> and compares it with the robot’s actual sensor readings stored in <code>loc.obs_range_data</code>.
                                            </p>
                                            <p>
                                                Each of the 18 individual sensor measurements is evaluated using the <code>loc.gaussian()</code> function, and their probabilities are multiplied together to get the final likelihood <strong>p(z∣x)</strong> for that pose. This likelihood is then multiplied with the prior belief from the prediction step (<code>bel_bar</code>) to compute the updated belief (<code>bel</code>).
                                            </p>
                                            <img class="img-fluid rounded w-100 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_update.png" alt="...">
                                        </p>
                                        <br>

                                        <h5>Result</h5>
                                        <p>
                                            After executing the full trajectory, I observe the localization results in the simulator:
                                        </p>
                                        <div class="row justify-content-center text-center">
                                            <div class="col-md-6">
                                              <img class="img-fluid rounded w-100 mb-4" src="assets/img/portfolio/local_traj0.png" alt="...">
                                            </div>
                                            <div class="col-md-6">
                                              <img class="img-fluid rounded w-100 mb-4" src="assets/img/portfolio/local_traj1.png" alt="...">
                                            </div>
                                        </div>
                                        <div class="row justify-content-center text-center">
                                            <div class="col-md-6">
                                              <img class="img-fluid rounded w-100 mb-4" src="assets/img/portfolio/local_traj2.png" alt="...">
                                            </div>
                                            <div class="col-md-6">
                                              <img class="img-fluid rounded w-100 mb-4" src="assets/img/portfolio/local_traj3.png" alt="...">
                                            </div>
                                        </div>
                                        <p>
                                            <ul>
                                                <li>🔴 Red – Odometry: Estimate from wheel encoders only. Accumulates drift quickly.</li>
                                                <li>🟩 Green – Ground Truth: Actual robot position in simulation.</li>    
                                                <li>🔵 Blue – Belief: Output of the Bayes filter. It closely tracks the green ground truth path.</li>
                                            </ul>
                                        </p>
                                        <p>
                                            The belief begins with uncertainty but gradually aligns with the actual path. The odometry alone diverges significantly, demonstrating the importance of using sensor data to correct the estimate.
                                        </p>
                                        <br>
                                        <p>The videos below display the process of the licalization:</p>
                                        <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                            <source src="assets/img/portfolio/local_video1.mp4" type="video/mp4">
                                        </video>
                                        <video width="400" controls class="d-block mx-auto mt-2 w-75">
                                            <source src="assets/img/portfolio/local_video2.mp4" type="video/mp4">
                                        </video>
                                        <p>Furthermore, the following is the data output during that process:</p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_stat1.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_stat2.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_stat3.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_stat4.png" alt="...">
                                        <br>

                                        <h5>Conclusion</h5>
                                        <p>
                                            In this lab, we successfully implemented grid localization using a Bayes filter with 3D state estimation. The algorithm integrates:
                                            <ul>
                                                <li>Odometry-based motion prediction</li>
                                                <li>Range-based sensor correction</li>
                                            </ul>
                                            This process allowed the robot to maintain an accurate belief of its pose in a simulated environment. The results demonstrate how probabilistic localization significantly outperforms raw odometry, especially in environments with many turns or obstacles.
                                        </p>
                                        <br>

                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio Modal 11-->
        <div class="portfolio-modal modal fade" id="portfolioModal11" tabindex="-1" aria-labelledby="portfolioModal11" aria-hidden="true">
            <div class="modal-dialog modal-xl">
                <div class="modal-content">
                    <div class="modal-header border-0">
                        <button class="btn-close" type="button" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body text-center pb-5">
                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <!-- Portfolio Modal - Title-->
                                    <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0">Lab 11<h3 class="text-warning">Grid Localization using Bayes Filter (Real)</h3></h2>
                                    <!-- Icon Divider-->
                                    <div class="divider-custom">
                                        <div class="divider-custom-line"></div>
                                        <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                        <div class="divider-custom-line"></div>
                                    </div>
                                    <!-- Portfolio Modal - Image-->
                                    <img class="img-fluid rounded mb-5 w-50" src="assets/img/portfolio/local_bel_00.png" alt="..." />
                                    <!-- Portfolio Modal - Text (Lab Writeup)-->
                                    <div class="text-start">
                                        <h5>Introduction</h5>
                                        <p>
                                            After successfully verifying the Bayes filter in simulation, this lab focuses on implementing the <strong>update step only</strong> on the real robot. 
                                            The prediction step was omitted due to noisy motion characteristics caused by variable friction, battery drift, and inconsistent wheel traction. 
                                            The goal was to observe how real-world sensor data (specifically ToF readings) influence belief updates when executing a 360° rotation in place.
                                        </p>
                                        <br>

                                        <h5>Lab Tasks</h5>
                                        <p>
                                            <strong>Bayes Filter (Simulation)</strong>
                                            <p>
                                                We began with <code>lab11_sim.ipynb</code>, which visualized the odometry, ground truth, and belief distribution. 
                                                This ensured the Bayes filter behaved correctly in simulation before transitioning to hardware.
                                            </p>
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_real_sim.png" alt="...">
                                        <br>

                                        <p>
                                            <strong>Localization on Real Robot</strong>
                                        </p>
                                        <p>
                                            The <code>LOCALIZATION</code> command was implemented in <strong>Arduino</strong>. The robot rotated 360° in place, 
                                            capturing 18 evenly spaced ToF readings (~20° apart), data being avg then stored in array which later send over to analysis through BLE. 
                                            To prevent missing the first data point, the 0° ToF reading was explicitly captured before the rotation began. 
                                        </p>
                                        <p>
                                            Using the <code>pid_orien_imu()</code> function implemented in previous lab, the robot was able to be control through PID to accurately turn 20 degree per step, 
                                            with an increment of 11 in its target angle. 
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_pid_orien.png" alt="...">
                                        <p>
                                            I also implemented an <code>reset_orientation()</code> function to ensure starting gyro angle is the starting angle, 
                                            to prevent the random spinning robot made when it starts orientation control during previous lab. 
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_reset_orien.png" alt="...">
                                        <p>
                                            After 18 data was collected, the robot stop its spinning and starting transmitting the ToF data.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_robot_code.png" alt="...">
                                        <br>

                                        <p>
                                            <strong>Python Implementation</strong>
                                        </p>
                                        <p>
                                            The <code>perform_observation_loop()</code> function is responsible for commanding the robot to perform a full 360° rotation while collecting 
                                            ToF sensor readings at evenly spaced angular intervals (approximately every 20°). 
                                        </p>
                                        <p>
                                            It begins by activating BLE notifications with a handler (<code>data_collect</code>) that parses incoming distance readings. 
                                            Then, it sends a <code>CMD.LOCALIZATION</code> command to the robot, along with PID parameter carefully tuned in Lab 9, 
                                            which uses PID control to rotate and pause at each angle to capture a ToF reading.
                                        </p>
                                        <p>
                                            To ensure the rotation completes and all data is received, the function uses 
                                            <code>asyncio.run(asyncio.sleep(20))</code> instead of <code>time.sleep</code>, allowing BLE messages to be processed without blocking the event loop.
                                        </p>
                                        <p>
                                            Once the rotation finishes, BLE notifications are stopped. The collected readings are converted from millimeters to meters and reshaped 
                                            into a column vector with shape (18×1), ready for the Bayes filter update step.
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_transmit_code.png" alt="...">
                                        <img class="img-fluid rounded w-100 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_obs_loop_code.png" alt="...">
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_bayes_code.png" alt="...">
                                        <br>

                                        <p>
                                            <strong>Ground Truth Comparison</strong>
                                        </p>
                                        <p>
                                            After running the Bayes Filter using the real robot at different location in the map, I obtained 3 beliefs for each spots:
                                        </p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_real_bel.png" alt="...">
                                        <p>
                                            Each result was compared to the belief distribution's peak from <code>loc.plot_update_step_data()</code>.
                                        </p>
                                        <br>

                                        <h5>Results</h5>
                                        <p>
                                            After 3 testing at each spot, I obtained the following results, where the green dots stand for theoretical result and the blue dots indicate actual position coordinates.
                                        </p>
                                        <p><strong>Position (0, 0):</strong></p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_bel_00.png" alt="...">
                                        <p><strong>Position (0, 3):</strong></p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_bel_0-3.png" alt="...">
                                        <p><strong>Position (-3, -2):</strong></p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_bel_-3-2.png" alt="...">
                                        <p><strong>Position(5, 3):</strong></p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_bel_53.png" alt="...">
                                        <p><strong>Position (5, -3):</strong></p>
                                        <img class="img-fluid rounded w-75 mb-4 d-block mx-auto mt-2" src="assets/img/portfolio/local_bel_5-3.png" alt="...">
                                        <p>
                                            The beliefs of the robot at all location are <strong>really correct</strong> compare to the robot truthPose, with a difference of less than 0.1. For location (-3, -2),(5, 3), and (5, -3), 
                                            there is 1 of the 3 Bayes Filter results that is not so accurate but is still near the truthPose area, which is likely due to the thick tape on the ground that hindered 
                                            the robot data collection process, making the ToF data no longer 20 degree even spacing and blocking the robot to rotat fully 360, which cause the shift in beliefs. The other 
                                            two beliefs of these location were not affected by the tapes and returned perfect belief locations.
                                        </p>
                                        <br>
                                        <br>

                                        <h5>Conclusion</h5>
                                        <p>
                                            The result of this lab presents a fairly accurate and reliable Bayes Filter belief output, showing that probabilistic localization with a Bayes filter can work effectively in real-world settings using ToF scans. 
                                            Despite sensor and motion noise, the robot localized accurately in all marked poses with some error trails affected by the map environment. Future improvements could include:
                                        </p>
                                        <ul>
                                            <li>Implementing the <code>get_pose()</code> function which integrate odometry data for future robot navigation</li>
                                            <li>Combining ToF and IMU data into a fused sensor model</li>
                                            <li>Visualizing the full belief heatmap over time</li>
                                        </ul>
                                        <br>

                                    </div>
                                    <!-- Close Button-->
                                    <button class="btn btn-primary mt-4" data-bs-dismiss="modal">
                                        <i class="fas fa-xmark fa-fw"></i>
                                        Close Window
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <!-- * *                               SB Forms JS                               * *-->
        <!-- * * Activate your form at https://startbootstrap.com/solution/contact-forms * *-->
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <script src="https://cdn.startbootstrap.com/sb-forms-latest.js"></script>
    </body>
</html>
